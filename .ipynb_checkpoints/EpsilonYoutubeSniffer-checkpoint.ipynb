{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook documents the construction of an application that performs three actions:\n",
    "\n",
    "1. Create a database containing snap shots of descriptor statistics for the all the videos on a number of channels on Youtube.\n",
    "\n",
    "2. Generate reports of pertinent information summarising some subselection of the videos retrieved.\n",
    "\n",
    "3. Perform time series analysis of of historical snap-shots.\n",
    "\n",
    "\n",
    "# Part 1\n",
    "\n",
    "\n",
    "Let's build the part of the application that talks to Youtube. There is a quickstart guide at:\n",
    "https://developers.google.com/youtube/v3/quickstart/python\n",
    "but we will mostly follow our own route using \n",
    "https://github.com/youtube/api-samples/blob/master/python/search.py\n",
    "\n",
    "We will also refer to \n",
    "\n",
    "https://developers.google.com/youtube/v3/docs/\n",
    "https://www.forgov.qld.gov.au/file/21896/download?token=mo1SpiZT \n",
    "\n",
    "We want to get a list of statistics for all videos for all channels in a given list. We'll need to make calls to at least 3 different resources:\n",
    "\n",
    "> \n",
    "**Channels**\n",
    "-  Needs: channel_name\n",
    "-  Returns: channel_id\n",
    "> \n",
    "**Videos**\n",
    "-  Needs: video_id\n",
    "-  Returns: Statistics on video\n",
    "> \n",
    "**PlaylistItems**\n",
    "-  Needs: playlist_id (derivable from channel_id)\n",
    "-  Returns: All videos_ids on that playlist\n",
    "    \n",
    "It's probably a good idea to first make an index of all the channel_ids of the channels that we want to survey. Usefully, each of these channel ids are only one character away from the id of the corresponding default playlist, which contains every video uploaded by that channel. \n",
    "\n",
    "There is a stackoverflow question which has a lot of useful information as to how we should structure our calls to the API: https://stackoverflow.com/questions/18953499/youtube-api-to-fetch-all-videos-on-a-channel/20795628\n",
    "\n",
    "From this, and a little consideration,  note a few important facts:\n",
    "1. Each call to an API resource returns at most 50 results, i.e., if a channel has uploaded 6000 videos (e.g., Khan Academy), to return every video_id we must make at least 120 calls to the API.\n",
    "2. You can iterate through this list of 6000 videos, 50 at a time, using the optional next_page token. \n",
    "3. You can actually only get a maximum of 500 videos using the next_page token.\n",
    "4. To get around the 500 max limitation, you can try to restrict your query by date of upload. This might take some trial and error if there were any periods of really high upload density.\n",
    "5. We'll need to do a big initial survey to get the video ids to date. After that the process will shift to maintenance of an existing database and should require much lower volumes of queries. However, If we want up-to-date information on videos, that will require a fair bit of resampling. API call quota may become an issue.\n",
    "\n",
    "\n",
    "There's only a few things that we really want:\n",
    "\n",
    "1. A list of all the videos on a given playlist (-time indexed? only by upload date?)\n",
    "2. Engagement statistics for each video. (-time indexed)\n",
    "\n",
    "\n",
    "This suggests the following tasks:\n",
    "\n",
    "1. Construct index of channel name and channel id. \n",
    "    - [] Manually construct list of names from Epsilon Stream channel list.\n",
    "    - [] Use channels API call to get channel id (or playlist ID) for each.\n",
    "    - [] Store list in file.\n",
    "    - [] Derive upload playlist ids from channel ids. \n",
    "\n",
    "2. Get list of all video ids on each channel -> Create video-list-updater\n",
    "    - [] routine to find total number of videos in playlist\n",
    "    - [] routine to create search request for playlist with bounded upload dates to restrict number of resuslts <500\n",
    "    - [] routine to take <500 result search and iterate through pagination\n",
    "    - [] routine to read search response and obtain all desired details\n",
    "    - [] routine to handle aggregation and quality control of requests (e.g. check for doubles/missing, repeat failed requests)\n",
    "    - [] routine to handle I/O of results\n",
    "\n",
    "3. Having gotten all video_ids for each channel, for each video:\n",
    "    - [] Asynchronously query video resource 50 ids at a time \n",
    "    - [] routine to extract \"status\" (all details desired) on every video\n",
    "    - [] routine to add each \n",
    "    - [] program to govern all these actions, reuse retry function and quality control from earlier\n",
    "\n",
    "To consider:\n",
    "What kinds of operations will I frequently be doing with this database?\n",
    "Should the database be snapshot based, or delta based, or a mix of the two?\n",
    "    \n",
    "\n",
    "\n",
    "### Calls to the Youtube API\n",
    "\n",
    "The following API call retrieves the video_ids of the first 50 videos, according to some order, of all videos posted by Khan Academy's channel on youtube. We take advantage of the fact that every channel has a default playlist resource, which contains a list of every video published on that channel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build  \n",
    "#We are currently building an authorisationless app, so we don't need oauth2client\n",
    "import os\n",
    "\n",
    "#put the textfile containing the apikey into the parent folder of your local git repo\n",
    "apikey_path = os.path.join(os.path.join(os.getcwd(),os.pardir),\"apikey.txt\")\n",
    "\n",
    "with open(apikey_path,\"rb\") as f:\n",
    "    apikey = f.readline()\n",
    "\n",
    "DEVELOPER_KEY = apikey\n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "playlist_id = \"UU4a-Gbdw7vOaccHmFo40b9g\" #This is the default playlist id for Khan Academy\n",
    "\n",
    "def get_youtube_playlist(playlist_id, max_results=50):\n",
    "    \n",
    "    youtube = build(serviceName = YOUTUBE_API_SERVICE_NAME,\n",
    "                    version = YOUTUBE_API_VERSION,\n",
    "                    developerKey = DEVELOPER_KEY)\n",
    "    \n",
    "    search_response = youtube.playlistItems().list(\n",
    "        playlistId = playlist_id,\n",
    "        part = 'snippet',\n",
    "        maxResults= max_results)\n",
    "    \n",
    "    return search_response.execute()\n",
    "\n",
    "playlist_result = get_youtube_playlist(playlist_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "videos = []\n",
    "\n",
    "for item in playlist_result[\"items\"]:\n",
    "    A=item[\"snippet\"][\"resourceId\"][\"videoId\"]\n",
    "    print A\n",
    "    videos.append(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following API call retrieves the \"id\" part of the channel resource that contains information about Khan Academy's channel. This is the channel id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_channel_id(channel_name, max_results=50):\n",
    "    \n",
    "    youtube = build(serviceName = YOUTUBE_API_SERVICE_NAME,\n",
    "                    version = YOUTUBE_API_VERSION,\n",
    "                    developerKey = DEVELOPER_KEY)\n",
    "    \n",
    "    query_response = youtube.channels().list(\n",
    "        forUsername = channel_name,\n",
    "        part = 'id',\n",
    "        maxResults= max_results)\n",
    "    \n",
    "    return query_response.execute()\n",
    "\n",
    "result = get_channel_id(\"khanacademy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print result[\"items\"][0][\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the channel id for Khan Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll take that list of 50 video ids and get a bunch of statistics for them using the Videos resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_video_ids = \",\".join(videos)\n",
    "test_video_ids = test_video_ids+\",\"+test_video_ids\n",
    "print test_video_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_video_statistics(video_ids, max_results=50):\n",
    "    \"\"\"returns a youtube API Video resource, containing details for a list of videos\"\"\"\n",
    "    \n",
    "    youtube = build(serviceName = YOUTUBE_API_SERVICE_NAME,\n",
    "                    version = YOUTUBE_API_VERSION,\n",
    "                    developerKey = DEVELOPER_KEY)\n",
    "    \n",
    "    search_response = youtube.videos().list(\n",
    "        id = video_ids,\n",
    "        part = 'snippet,statistics',\n",
    "        maxResults= max_results)\n",
    "    \n",
    "    return search_response.execute()\n",
    "\n",
    "video_result = get_video_statistics(test_video_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now got some statistics for the 50 videos captured above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for item in video_result[\"items\"]:\n",
    "    A = float(item[\"statistics\"][\"viewCount\"])\n",
    "    B = float(item[\"statistics\"][\"likeCount\"])\n",
    "    C = float(B/A)\n",
    "    print int(A), int(B), C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we should figure out how to use pagination of results. We'll choose the default playlist of 3Blue1Brown as our testing grounds. This channel currently has about 70 videos, so we should expect just 2 pages of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bb_channelId = \"UCYO_jab_esuFRV4b17AJtAw\"\n",
    "bb_playlist_id = \"UUYO_jab_esuFRV4b17AJtAw\"\n",
    "\n",
    "def get_next_page_token(response):\n",
    "    \"\"\"Given json Youtube API response, return next_page token\"\"\"\n",
    "    try:\n",
    "        return response[\"nextPageToken\"] \n",
    "    except KeyError:\n",
    "        return None\n",
    "    \n",
    "def num_results(response):\n",
    "    \"Get number of results to query\"\n",
    "    return int(response[\"pageInfo\"][\"totalResults\"])\n",
    "\n",
    "\n",
    "def add_playlist_video_details(response, dataset):\n",
    "    \"\"\"reads a playlist call response and extracts dict of desired information\"\"\" \n",
    "    for item in response[\"items\"]:\n",
    "        video_id = item[\"snippet\"][\"resourceId\"][\"videoId\"]\n",
    "        dataset[video_id] = {\"likes\":1, \"dislikes\":3, \"views\":20} #arbitrary\n",
    "    return dataset\n",
    "\n",
    "def add_search_video_details(response, dataset):\n",
    "    \"\"\"reads a search call response and extracts dict of desired information\"\"\" \n",
    "    for item in response[\"items\"]:\n",
    "        print item\n",
    "        video_id = item[\"id\"][\"videoId\"]\n",
    "        dataset[video_id] = {\"likes\":1, \"dislikes\":3, \"views\":20} #arbitrary\n",
    "    return dataset\n",
    "\n",
    "def get_paginated_playlist(playlist_id, max_results=50, first_token=None):\n",
    "    \n",
    "    youtube = build(serviceName = YOUTUBE_API_SERVICE_NAME,\n",
    "                    version = YOUTUBE_API_VERSION,\n",
    "                    developerKey = DEVELOPER_KEY)\n",
    "    \n",
    "    token = None\n",
    "    \n",
    "    search_response = youtube.playlistItems().list(\n",
    "        playlistId = playlist_id,\n",
    "        part = 'snippet',\n",
    "        maxResults= max_results,\n",
    "        pageToken =None)\n",
    "    \n",
    "    result = search_response.execute()\n",
    "    \n",
    "    token =  get_next_page_token(result)\n",
    "    \n",
    "    search_response2 = youtube.playlistItems().list(\n",
    "        playlistId = playlist_id,\n",
    "        part = 'snippet',\n",
    "        maxResults= max_results,\n",
    "        pageToken = token\n",
    "        )\n",
    "    \n",
    "    return search_response2.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bb_results = get_youtube_playlist(bb_playlist_id)\n",
    "bb_results_2 = get_paginated_playlist(bb_playlist_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_video_details_from_playlist(playlist_id):\n",
    "    \"\"\"Gets all videos on a youtube playlist by collecting paginated results\"\"\"\n",
    "    \n",
    "    youtube = build(serviceName = YOUTUBE_API_SERVICE_NAME,\n",
    "                    version = YOUTUBE_API_VERSION,\n",
    "                    developerKey = DEVELOPER_KEY)\n",
    "    dataset = {}\n",
    "    \n",
    "    query = youtube.playlistItems().list(\n",
    "                playlistId = playlist_id,\n",
    "                part = 'snippet',\n",
    "                pageToken=None)\n",
    "    first_page = query.execute()\n",
    "    \n",
    "    token = get_next_page_token(first_page)\n",
    "    dataset = add_playlist_video_details(first_page, dataset) \n",
    "    #maybe create a dataset class with some nice methods?\n",
    "    \n",
    "    while token:\n",
    "        \n",
    "        query = youtube.playlistItems().list(\n",
    "            playlistId = playlist_id,\n",
    "            part = 'snippet',\n",
    "            pageToken = token\n",
    "            )\n",
    "        page_response = query.execute()\n",
    "        \n",
    "        token = get_next_page_token(page_response)\n",
    "        dataset = add_playlist_video_details(page_response, dataset) \n",
    "    \n",
    "    return dataset\n",
    "\n",
    "bb_results_3 = get_video_details_from_playlist(bb_playlist_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_results_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bb_results_3) #expected = 67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try to survey a larger channel. The h3h3 productions main channel has 289 videos, we'll try to retrieve the id's of all 289 (this figure read manually from the channel page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h3h3 = \"UUDWIvJwLJsE4LG1Atne2blQ\"\n",
    "h3h3c = \"UCDWIvJwLJsE4LG1Atne2blQ\"\n",
    "h3h3_results = get_video_details_from_playlist(h3h3) #288 of 289, missing one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(h3h3_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queries using PlaylistItems have a hard cap of 500 items. We'll need to use the Search resource to get beyond this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_videos_from_channel_using_search(channel_id):\n",
    "    \"\"\"Gets all videos on a youtube channel by collecting paginated results\"\"\"\n",
    "    \n",
    "    youtube = build(serviceName = YOUTUBE_API_SERVICE_NAME,\n",
    "                    version = YOUTUBE_API_VERSION,\n",
    "                    developerKey = DEVELOPER_KEY)\n",
    "    dataset = {}\n",
    "    \n",
    "    query = youtube.search().list(\n",
    "                q=\"\",\n",
    "                part = 'snippet,id',\n",
    "                pageToken=None,\n",
    "                channelId = channel_id)\n",
    "    first_page = query.execute()\n",
    "    \n",
    "    token = get_next_page_token(first_page)\n",
    "    dataset = add_search_video_details(first_page, dataset) \n",
    "    #maybe create a dataset class with some nice methods?\n",
    "    \n",
    "    while token:\n",
    "        \n",
    "        query = youtube.search().list(\n",
    "            q=\"\",\n",
    "            part = 'snippet,id',\n",
    "            pageToken = token,\n",
    "            channelId = channel_id\n",
    "            )\n",
    "        page_response = query.execute()\n",
    "        \n",
    "        token = get_next_page_token(page_response)\n",
    "        dataset = add_search_video_details(page_response, dataset) \n",
    "    \n",
    "    return dataset\n",
    "\n",
    "h3h3_results = get_videos_from_channel_using_search(h3h3c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asynchronous requests to the Youtube API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually we will need to increase efficiency by performing our surveys asynchronously, below is an example using trollius, the (now deprecated) python 2.7 port of the python 3.4+ native asynchronous programming module, asyncio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import trollius as asyncio\n",
    "from trollius import From\n",
    "\n",
    "@asyncio.coroutine\n",
    "def factorial(name, number):\n",
    "    f = 1\n",
    "    for i in range(2, number + 1):\n",
    "        print(\"Task %s: Compute factorial(%d)...\" % (name, i))\n",
    "        yield From(asyncio.sleep(1))\n",
    "        f *= i\n",
    "    print(\"Task %s completed! factorial(%d) is %d\" % (name, number, f))\n",
    "\n",
    "loop = asyncio.new_event_loop()\n",
    "asyncio.set_event_loop(loop)\n",
    "tasks = [\n",
    "    asyncio.async(factorial(\"A\", 8)),\n",
    "    asyncio.async(factorial(\"B\", 3)),\n",
    "    asyncio.async(factorial(\"C\", 4))]\n",
    "loop.run_until_complete(asyncio.wait(tasks))\n",
    "loop.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know how to use this in conjunction with the Youtube API, so I may have to either/both switch to python 3 or/and write my own POST requests to the API.\n",
    "\n",
    "Fortunately, there's another asynchronous programming package for python -  gevent. Below we add to a short example from http://sdiehl.github.io/gevent-tutorial/ which demonstrates the speed up from non-blocking code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gevent\n",
    "import random\n",
    "import time\n",
    "\n",
    "rands = [random.randint(0,2)*0.1 for i_ in range(10)]\n",
    "\n",
    "tic = lambda t: (time.time() - t)*1000 #time since t in ms \n",
    "\n",
    "def task(pid, wait_t):\n",
    "    \"\"\"\n",
    "    Some non-deterministic task\n",
    "    \"\"\"\n",
    "    task_start = time.time()\n",
    "    gevent.sleep(wait_t)\n",
    "    print 'Task {0} done at {1:1.2f} ms, took {2:1.2f} ms'.format(pid, tic(start),tic(task_start))\n",
    "\n",
    "def synchronous():\n",
    "    for pid, wait_t in enumerate(rands):\n",
    "        task(pid, wait_t)\n",
    "\n",
    "def asynchronous():\n",
    "    threads = [gevent.spawn(task, *(pid,wait_t)) for pid, wait_t in enumerate(rands)]\n",
    "    gevent.joinall(threads)\n",
    "\n",
    "start = time.time()  \n",
    "print 'Synchronous:'\n",
    "synchronous()\n",
    "print \"All done in {:2.1f} ms\".format(tic(start))\n",
    "\n",
    "start = time.time()\n",
    "print 'Asynchronous:'\n",
    "asynchronous()\n",
    "print \"All done in {:2.1f} ms\".format(tic(start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second set of tasks are clearly happening at the same time, but there's some extra processing time sneaking in somewhere. Note that the individual working time of each task is the same across async and sync, but that async does them all (nearly) at once, as desired!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gevent.monkey\n",
    "gevent.monkey.patch_socket()\n",
    "import time\n",
    "import gevent\n",
    "# import urllib2\n",
    "import requests\n",
    "# import simplejson as json\n",
    "\n",
    "tic = lambda t: (time.time() -t)*1000 # time since t in ms\n",
    "\n",
    "target_url = 'https://www.googleapis.com/youtube/v3/search?part=snippet&channelId=UCDWIvJwLJsE4LG1Atne2blQ&maxResults=50&type=video&key={0}'.format(DEVELOPER_KEY)\n",
    "# target_url = 'https://jsonplaceholder.typicode.com/posts/1'\n",
    "\n",
    "def fetch(pid):\n",
    "    start = time.time()\n",
    "    response = requests.get(target_url)\n",
    "    json_result = response.json()\n",
    "    print 'Process {}: {:.2f}'.format(pid, tic(start))\n",
    "    return json_result\n",
    "    return None\n",
    "\n",
    "def synchronous():\n",
    "    for i in range(1,5):\n",
    "        fetch(i)\n",
    "\n",
    "def asynchronous():\n",
    "    threads = []\n",
    "    for i in range(1,5):\n",
    "        threads.append(gevent.spawn(fetch, i))\n",
    "    return gevent.joinall(threads)\n",
    "\n",
    "p_start = time.time()\n",
    "print 'Synchronous:'\n",
    "synchronous()\n",
    "print \"All done in {:.2f}\".format(tic(p_start))\n",
    "\n",
    "p_start = time.time()\n",
    "print 'Asynchronous:'\n",
    "responses = asynchronous()\n",
    "print \"All done in {:.2f}\".format(tic(p_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These requests are sort-of being sent asynchronously (since they are returned out of order), but it seems that requests is blocking IO somewhere. Any difference in total completion time between async and sync processes is just due to the random response time of the server.\n",
    "\n",
    "Fortunately, there is grequests for async requests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nbgrequests as grequests   #changed one tiny boolean\n",
    "import requests\n",
    "import time \n",
    "import os\n",
    "\n",
    "#put the textfile containing the apikey into the parent folder of your local git repo\n",
    "apikey_path = os.path.join(os.path.join(os.getcwd(),os.pardir),\"apikey.txt\")\n",
    "\n",
    "with open(apikey_path,\"rb\") as f:\n",
    "    apikey = f.readline()\n",
    "\n",
    "def tic(t):\n",
    "    # time since t in ms\n",
    "    return (time.time() -t)*1000 \n",
    "\n",
    "# n=2\n",
    "# urls = [url_maker(20+i) for i in range(0,n)]\n",
    "\n",
    "##Example 1: 7 http requests, repeating some targets. Works asynchronously\n",
    "# urls = [\n",
    "#     'http://www.heroku.com',\n",
    "#     'http://python-tablib.org',\n",
    "#     'http://httpbin.org',\n",
    "#     'http://python-requests.org',\n",
    "#     'http://python-requests.org',\n",
    "#     'http://python-requests.org',\n",
    "#     'http://python-requests.org',\n",
    "#       ]\n",
    "\n",
    "###Example 2: 4 https requests, repeating some targets. \n",
    "# urls = [\n",
    "#     'https://www.heroku.com',\n",
    "#     'https://httpbin.org',\n",
    "#     'https://httpbin.org',\n",
    "#     'https://httpbin.org',\n",
    "#       ]\n",
    "\n",
    "###Example 3: 3 Youtube v3 API search queries for the same channel, 3 different search terms. These queries all successfully return results, but do not run asynchronously\n",
    "urls =['https://www.googleapis.com/youtube/v3/search?part=snippet&channelId=UCDWIvJwLJsE4LG1Atne2blQ&key={0}&q=1'.format(apikey)\n",
    "        ,'https://www.googleapis.com/youtube/v3/search?part=snippet&channelId=UCDWIvJwLJsE4LG1Atne2blQ&key={0}&q=2'.format(apikey)\n",
    "        ,'https://www.googleapis.com/youtube/v3/search?part=snippet&channelId=UCDWIvJwLJsE4LG1Atne2blQ&key={0}&q=3'.format(apikey)\n",
    "        ]\n",
    "\n",
    "###Example 4: 3 Youtube v3 API search queries for 3 different channels.\n",
    "# urls = ['https://www.googleapis.com/youtube/v3/search?part=snippet&channelId=UCDWIvJwLJsE4LG1Atne2blQ&key={0}&q=1'.format(apikey)\n",
    "#         ,'https://www.googleapis.com/youtube/v3/search?part=snippet&channelId=UC4a-Gbdw7vOaccHmFo40b9g&key={0}&q=1'.format(apikey)\n",
    "#         ,'https://www.googleapis.com/youtube/v3/search?part=snippet&channelId=UCjwOWaOX-c-NeLnj_YGiNEg&key={0}&q=1'.format(apikey)\n",
    "#         ]\n",
    "\n",
    "\n",
    "def fetch(url):\n",
    "    start = time.time()\n",
    "    response = requests.get(url)\n",
    "    result = response.status_code\n",
    "    print 'Process {}: {:.2f}'.format(url, tic(start))\n",
    "    return result\n",
    "\n",
    "def async_fetch(unsent_requests):\n",
    "    start = time.time()\n",
    "    responses = grequests.map(unsent_requests)\n",
    "    results = [response.status_code for response in responses]\n",
    "    print 'Async Process: {:.2f}'.format( tic(start))\n",
    "    return results\n",
    "\n",
    "def synchronous(urls):\n",
    "    responses = [fetch(i) for i in urls]\n",
    "    return responses\n",
    "        \n",
    "def asynchronous(urls):\n",
    "    unsent_requests = [grequests.get(url) for url in urls]\n",
    "    responses = async_fetch(unsent_requests)\n",
    "    return responses\n",
    "\n",
    "    \n",
    "p_start = time.time()\n",
    "print 'Synchronous:'\n",
    "sync_results = synchronous(urls)\n",
    "print \"All done in {:.2f}\".format(tic(p_start))\n",
    "print sync_results\n",
    "    \n",
    "p_start = time.time()\n",
    "print 'Asynchronous:'\n",
    "async_results = asynchronous(urls)\n",
    "print \"All done in {:.2f}\".format(tic(p_start))   \n",
    "print async_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The async queries are returning in about the time it takes to do one sync query, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's begin writing the workhorse code that will actually do the things we want with youtube!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "227px",
    "left": "589.091px",
    "right": "20px",
    "top": "120px",
    "width": "345px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
