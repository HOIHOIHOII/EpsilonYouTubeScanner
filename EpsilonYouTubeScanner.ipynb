{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import requests\n",
    "import nbgrequests as grequests   #changed one tiny boolean\n",
    "import datetime\n",
    "import math\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import re\n",
    "from __future__ import division\n",
    "from itertools import groupby\n",
    "\n",
    "#put the textfile containing the apikey into the parent folder of your local git repo\n",
    "apikey_path = os.path.join(os.path.join(os.getcwd(),os.pardir),\"apikey.txt\")\n",
    "\n",
    "with open(apikey_path,\"rb\") as f:\n",
    "    apikey = f.readline()\n",
    "\n",
    "#sending queries\n",
    "\n",
    "def send_query(query):\n",
    "    '''gets API response for a single query'''\n",
    "    response = requests.get(query)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def async_fetch(requests):\n",
    "    unsent_requests = [grequests.get(r) for r in requests]\n",
    "    responses = grequests.map(unsent_requests)\n",
    "    status_codes = [response.status_code for response in responses]\n",
    "    print status_codes\n",
    "    results = [response.json() for response in responses]\n",
    "    return results\n",
    "\n",
    "#tools\n",
    "\n",
    "def grouper(items,group_size):\n",
    "    \"\"\"group list of items into sublists of length group_size\"\"\"\n",
    "    n=len(items)\n",
    "    num_groups = int(math.ceil(n / group_size))\n",
    "    groups = [items[i::num_groups] for i in range(num_groups)]\n",
    "    return groups\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    \"\"\"flattens a list of lists into a list\"\"\"\n",
    "    return [x for y in list_of_lists for x in y]\n",
    "\n",
    "\n",
    "#query construction\n",
    "\n",
    "def htmlify(s):\n",
    "    \"\"\"alter string to replace html disallowed characters in Youtube API query\"\"\"\n",
    "    substitutions = {\",\":\"%2C\"}\n",
    "    for a,b in substitutions.items():\n",
    "        s = s.replace(a,b)\n",
    "    return s\n",
    "\n",
    "def write_request(resource, params):\n",
    "    \"\"\"take resource name and a list of (key,value) pairs, write a Youtube API request\n",
    "    \n",
    "    attributes\n",
    "    ----------\n",
    "    resource: string                  name of the desired Youtube resource\n",
    "    params  : list of key,val pairs   other parameters for the query  \n",
    "    \n",
    "    params is encoded as a list of key val pairs because code below needed a predictable order\n",
    "    in params and dict doesn't have that.\"\"\"\n",
    "    \n",
    "    query = \"https://www.googleapis.com/youtube/v3/\"    #all queries begin thus\n",
    "    query = query+\"{0}\".format(resource)\n",
    "    \n",
    "    for key, value in params:\n",
    "        if type(value) is list or type(value) is tuple: #special handling of lists, tuples\n",
    "            comma_sep_vals = \",\".join(value)\n",
    "            term = \"{0}={1}\".format(key,comma_sep_vals)\n",
    "        elif type(value) is datetime.datetime:          #special handling of datetimes, must be RFC3339 format\n",
    "            term = \"{0}={1}\".format(key,to_RFC3339(value))\n",
    "        else:\n",
    "            term = \"{0}={1}\".format(key,value)\n",
    "            \n",
    "        if key == \"part\":\n",
    "            query = query+\"?{0}\".format(term)\n",
    "        else:\n",
    "            query = query+\"&{0}\".format(term)\n",
    "    return htmlify(query)\n",
    "\n",
    "\n",
    "\n",
    "###low level fetch functions\n",
    "\n",
    "def make_query_Ch_vid_count(channelId):\n",
    "    '''make a query to get the number of videos in a given playlist'''\n",
    "    \n",
    "    query_params = [(\"part\",[\"statistics\"]),\n",
    "                    (\"id\",channelId),\n",
    "                    (\"maxResults\",50),\n",
    "                    (\"key\",apikey)\n",
    "                   ]\n",
    "#     s = 'https://www.googleapis.com/youtube/v3/channels?part=statistics&id={0}&key={1}'.format(channelId, apikey)\n",
    "    s = write_request(\"channels\", query_params)\n",
    "    return s\n",
    "\n",
    "def read_reply_get_Ch_vid_count(response):\n",
    "    '''reads Ch_vid_count response and returns number of videos in the playlist'''\n",
    "    return response[\"items\"][0][\"statistics\"][\"videoCount\"]\n",
    "\n",
    "\n",
    "def ch_v_count(channelId):\n",
    "    \"\"\"return number of videos on a channel\"\"\"\n",
    "    q = make_query_Ch_vid_count(channelId)\n",
    "    r = send_query(q)\n",
    "    return int(read_reply_get_Ch_vid_count(r))\n",
    "\n",
    "\n",
    "def make_query_Ch_srch_response_count_between_dates(channelId, publishedAfter, publishedBefore):\n",
    "    '''make a query to get the number of videos on a given channel between two dates, by counting items in results.\n",
    "    \n",
    "    attributes\n",
    "    ----------\n",
    "    channelId : string       Youtube Channel id\n",
    "    publishedAfter  : datetime    start of date interval\n",
    "    publishedBefore : datetime    end of date interval \n",
    "    \n",
    "    Do not trust totalResults : val in response for the true number of hits. This number is unreliable. \n",
    "    e.g. #results(fn(chId,d1,d2))+ #results(fn(chId,d2,d3)) =/= #results(fn(chId,d1,d3))'''\n",
    "    \n",
    "    query_params = [(\"part\",[\"snippet\"]),\n",
    "                    (\"publishedAfter\", publishedAfter),\n",
    "                    (\"publishedBefore\", publishedBefore),\n",
    "                    (\"channelId\",channelId),\n",
    "                    (\"type\",\"video\"),\n",
    "                    (\"order\",\"date\"),                      #probably not necessary\n",
    "                    (\"maxResults\",50),\n",
    "                    (\"key\",apikey)\n",
    "                   ]\n",
    "#     s = 'https://www.googleapis.com/youtube/v3/channels?part=statistics&id={0}&key={1}'.format(channelId, apikey)\n",
    "    s = write_request(\"search\", query_params)\n",
    "    return s\n",
    "\n",
    "def read_reply_Ch_srch_response_count_between_dates(response):\n",
    "    \"\"\"reads Ch_srch_response_count_between_dates response and gets number of search results\"\"\"\n",
    "    return len(response[\"items\"])    #maxResults for request should be set to 50\n",
    "\n",
    "def lt_50_vids(channelId, publishedAfter, publishedBefore):\n",
    "    q = make_query_Ch_srch_response_count_between_dates(\n",
    "            channelId, publishedAfter, publishedBefore)\n",
    "    response = send_query(q)\n",
    "    n = read_reply_Ch_srch_response_count_between_dates(response)\n",
    "    return n < 50\n",
    "\n",
    "def make_query_get_Ch_creation_date(channelId):\n",
    "    '''make a query to get the creation date of a channel'''\n",
    "    \n",
    "    query_params = [(\"part\",[\"snippet\"]),\n",
    "                    (\"id\",channelId),\n",
    "                    (\"maxResults\",50),\n",
    "                    (\"key\",apikey)\n",
    "                   ]\n",
    "    s = write_request(\"channels\", query_params)\n",
    "#     s = 'https://www.googleapis.com/youtube/v3/channels?part=snippet&id={0}&key={1}'.format(channelId, apikey)\n",
    "    return s\n",
    "\n",
    "def read_reply_get_Ch_creation_date(response):\n",
    "    '''reads get_Ch_creation_date response and returns creation date of a channel as a datetime obj'''\n",
    "    return read_str_RFC3339(response[\"items\"][0][\"snippet\"][\"publishedAt\"])\n",
    "\n",
    "def ch_creation_date(channelId):\n",
    "    \"\"\"return creation date of channel\"\"\"\n",
    "    q = make_query_get_Ch_creation_date(channelId)\n",
    "    r = send_query(q)\n",
    "    return read_reply_get_Ch_creation_date(r)\n",
    "\n",
    "def make_query_get_video_details(video_ids):\n",
    "    \"\"\"fetches viewing statistics for a list of videos.\n",
    "    \n",
    "    attributes\n",
    "    ------------\n",
    "    video_ids : list     list of video ids, each an 11 character string\"\"\"\n",
    "    \n",
    "    \n",
    "    video_ids = \",\".join(video_ids)  #transform to string for query\n",
    "    \n",
    "    query_params = [(\"part\",[\"snippet\",\n",
    "                             \"statistics\",\n",
    "                             \"contentDetails\",\n",
    "                            ]),\n",
    "                    (\"id\", video_ids),\n",
    "                    (\"type\",\"video\"),\n",
    "                    (\"maxResults\",50),\n",
    "                    (\"key\",apikey)\n",
    "                   ]\n",
    "#   s = 'https://www.googleapis.com/youtube/v3/channels?part=statistics&id={0}&key={1}'.format(channelId, apikey)\n",
    "    s = write_request(\"videos\", query_params)\n",
    "    return s\n",
    "\n",
    "def schedule(publishedAt):\n",
    "    \"\"\"schedule next timeseries sample\"\"\"\n",
    "    now = datetime.datetime.utcnow()\n",
    "    video_age_days = (now - publishedAt).days\n",
    "    if video_age_days < 10:\n",
    "        nextupdate = now + datetime.timedelta(hours=6)\n",
    "    else:\n",
    "        nextupdate = now + datetime.timedelta(days=5)\n",
    "    return nextupdate\n",
    "\n",
    "#+++++++++++++++++++++\n",
    "\n",
    "def prepare_video_request(videoIds):\n",
    "    \"\"\"create  youtube API request for videos data.\n",
    "    \n",
    "    Collects both timeseries and meta data\"\"\"\n",
    "    query_params = [(\"part\",[\"snippet\",\"statistics\",\"topicDetails\",\"contentDetails\"]),\n",
    "                    (\"id\",videoIds),\n",
    "                    (\"maxResults\",50),\n",
    "                    (\"key\",apikey)\n",
    "                   ]\n",
    "    s = write_request(\"videos\", query_params)\n",
    "    return s\n",
    "\n",
    "def extract_videos_meta(response):\n",
    "    \"\"\"extract a list of video_meta records\n",
    "    \n",
    "    :param unicode response: A reply from the YouTube API, having queried a video resource\"\"\"\n",
    "    \n",
    "    new_videos_meta = []\n",
    "    for video in response[\"items\"]:\n",
    "        #metaData content\n",
    "        data = {}\n",
    "        data[\"videoId\"] = video[\"id\"]\n",
    "        data[\"title\"] = video[\"snippet\"][\"title\"]\n",
    "        data[\"channelId\"] = video[\"snippet\"][\"channelId\"]\n",
    "        data[\"publishedAt\"] = read_str_RFC3339(video[\"snippet\"][\"publishedAt\"])\n",
    "        data[\"description\"] = video[\"snippet\"].get(\"description\", None)\n",
    "        data[\"tags\"] = video[\"snippet\"].get(\"tags\", None)\n",
    "        data[\"duration\"] = convert_YT_dur(video[\"contentDetails\"][\"duration\"])\n",
    "        data[\"lastUpdate\"] = datetime.datetime.utcnow()\n",
    "        data[\"nextUpdate\"] = schedule(data[\"publishedAt\"])\n",
    "        new_videos_meta.append(data)\n",
    "    return new_videos_meta\n",
    "        \n",
    "def extract_videos_timeseries(response):\n",
    "    \"\"\"extract a list of video_timeseries records\n",
    "    \n",
    "    :param unicode response: A reply from the YouTube API, having queried a video resource\"\"\"\n",
    "    \n",
    "    new_videos_timeseries = []\n",
    "    for video in response[\"items\"]:\n",
    "        #timeseries content\n",
    "        data = {}\n",
    "        data[\"videoId\"] = video[\"id\"]\n",
    "        data[\"viewCount\"] = video[\"statistics\"].get(\"viewCount\",0)\n",
    "        data[\"likeCount\"] = video[\"statistics\"].get(\"likeCount\",0)\n",
    "        data[\"dislikeCount\"] = video[\"statistics\"].get(\"dislikeCount\",0)\n",
    "        data[\"favoriteCount\"] = video[\"statistics\"].get(\"favoriteCount\",0)    \n",
    "        data[\"commentCount\"] = video[\"statistics\"].get(\"commentCount\",0) \n",
    "        data[\"sampleTime\"] = datetime.datetime.utcnow()\n",
    "        #for operational purposes, not stored as ts data\n",
    "        data[\"publishedAt\"] = read_str_RFC3339(video[\"snippet\"][\"publishedAt\"])\n",
    "        new_videos_timeseries.append(data)\n",
    "    return new_videos_timeseries\n",
    "\n",
    "def get_video_data(videoIds):\n",
    "    \"\"\"get youtube metadata and timeseries for a list of videos\"\"\"\n",
    "    #get the data from youtube API for all the channels asynchronously\n",
    "    video_groups = grouper(videoIds, 50)\n",
    "    requests = [prepare_video_request(group) for group in video_groups]\n",
    "    responses = async_fetch(requests)\n",
    "    \n",
    "    #read responses\n",
    "    metadata = [extract_videos_meta(response) for response in responses]\n",
    "    timeseries = [extract_videos_timeseries(response) for response in responses]\n",
    "    \n",
    "    #TODO - log any failures\n",
    "    return flatten(metadata), flatten(timeseries)\n",
    "\n",
    "#+++++++++++++++++++++\n",
    "def extract_channels_meta(response):\n",
    "    \"\"\"extract a list of channel_meta records\n",
    "    \n",
    "    :param unicode response: A reply from the YouTube API, having queried a channel resource\"\"\"\n",
    "    \n",
    "    new_channels = []\n",
    "    now = datetime.datetime.utcnow()\n",
    "    \n",
    "    for channel in response[\"items\"]:\n",
    "        data = {} #initialise\n",
    "        #metaData content\n",
    "        data[\"channelId\"] = channel[\"id\"]\n",
    "        data[\"title\"] = channel[\"snippet\"][\"title\"]\n",
    "        data[\"description\"] = channel[\"snippet\"].get(\"description\", None)\n",
    "        data[\"publishedAt\"] = read_str_RFC3339(channel[\"snippet\"][\"publishedAt\"])\n",
    "        data[\"containsNonMathsVideos\"] = False #default\n",
    "        data[\"topicIds\"] = channel[\"topicDetails\"].get(\"topicIds\", None)\n",
    "        data[\"lastUpdate\"] = now\n",
    "        data[\"nextUpdate\"] = schedule(data[\"publishedAt\"])\n",
    "        new_channels.append(data)\n",
    "    return new_channels\n",
    "    \n",
    "def extract_channels_timeseries(response):\n",
    "    \"\"\"extract a list of channel_timeseries records\n",
    "    \n",
    "    :param unicode response: A reply from the YouTube API, having queried a channel resource\"\"\"\n",
    "    \n",
    "    new_channels_timeseries = []\n",
    "    now = datetime.datetime.utcnow()\n",
    "    \n",
    "    for channel in response[\"items\"]:\n",
    "        data = {} #initialise\n",
    "        #timeseries content\n",
    "        data[\"channelId\"] = channel[\"id\"]\n",
    "        data[\"viewCount\"] = channel[\"statistics\"][\"viewCount\"]\n",
    "        data[\"commentCount\"] = channel[\"statistics\"][\"commentCount\"]\n",
    "        data[\"subscriberCount\"] = channel[\"statistics\"][\"subscriberCount\"]\n",
    "        data[\"videoCount\"] = channel[\"statistics\"][\"videoCount\"]\n",
    "        data[\"sampleTime\"] = now\n",
    "        \n",
    "        new_channels_timeseries.append(data)\n",
    "    return new_channels_timeseries\n",
    "    \n",
    "def get_channel_data(channelIds):\n",
    "    \"\"\"get youtube metadata and timeseries for a list of channels\"\"\"\n",
    "    #get the data from youtube API for all the channels asynchronously\n",
    "    channel_groups = grouper(channelIds, 50)\n",
    "    requests = [prepare_channel_request(group) for group in channel_groups]\n",
    "    responses = async_fetch(requests)\n",
    "    \n",
    "    #read responses\n",
    "    metadata = [extract_channels_meta(response) for response in responses]\n",
    "    timeseries = [extract_channels_timeseries(response) for response in responses]\n",
    "    \n",
    "    #do - load metadata into database\n",
    "    #TODO - log any failures\n",
    "    return flatten(metadata), flatten(timeseries)\n",
    "    \n",
    "#+++++++++++++++++\n",
    "\n",
    "def make_query_Ch_get_vids_between_dates(channelId, publishedAfter, publishedBefore):\n",
    "    '''make a query to get the videos on a given channel between two dates.\n",
    "    \n",
    "    attributes\n",
    "    ----------\n",
    "    channelId : string       Youtube Channel id\n",
    "    publishedAfter  : datetime    start of date interval\n",
    "    publishedBefore : datetime    end of date interval \n",
    "    \n",
    "    Do not trust totalResults : val in response for the true number of search results.  \n",
    "    e.g. #results(fn(chId,d1,d2))+ #results(fn(chId,d2,d3)) =/= #results(fn(chId,d1,d3))'''\n",
    "    \n",
    "    query_params = [(\"part\",[\"snippet\"]),\n",
    "                    (\"publishedAfter\", publishedAfter),\n",
    "                    (\"publishedBefore\", publishedBefore),\n",
    "                    (\"channelId\",channelId),\n",
    "                    (\"type\",\"video\"),\n",
    "                    (\"maxResults\",50),\n",
    "                    (\"key\",apikey)\n",
    "                   ]\n",
    "#     s = 'https://www.googleapis.com/youtube/v3/channels?part=statistics&id={0}&key={1}'.format(channelId, apikey)\n",
    "    s = write_request(\"search\", query_params)\n",
    "    return s\n",
    "\n",
    "def extract_video_ids(response): \n",
    "    \"\"\"take a response and extract a list of video_ids.\n",
    "    \n",
    "    attributes:\n",
    "    -----------\n",
    "    response: string     \n",
    "    Youtube search query response created by fn make_query_Ch_get_vids_between_dates\"\"\"\n",
    "    video_ids = [video[\"id\"][\"videoId\"] for video in response[\"items\"]]\n",
    "    return video_ids\n",
    "\n",
    "\n",
    "#datetime manipulation\n",
    "\n",
    "def to_RFC3339(datetime_obj):\n",
    "    \"\"\"format a datetime object in RFC3339 format, e.g. 1999-11-20T04:34:11Z\"\"\"\n",
    "    return datetime_obj.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "def read_str_RFC3339(datetime_str):\n",
    "    \"\"\"parse a RFC3339 formatted string, e.g. 1999-11-20T04:34:11Z\"\"\"\n",
    "    return datetime.datetime.strptime(datetime_str,\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "def convert_YT_dur(YT_dur):\n",
    "    \"\"\"convert a YT video duration string into a timedelta object\n",
    "    \n",
    "    examples PT2M30S, PT1H3M37S, PT6S, PT2M, PT4M39S\"\"\"\n",
    "    pattern  = r\"PT(?:(?P<hours>[0-9]+)*H)?(?:(?P<minutes>[0-9]+)*M)?(?:(?P<seconds>[0-9]+)*S)?\"\n",
    "    m = re.search(pattern, YT_dur)\n",
    "    mgroup =  {\"hours\": m.group(\"hours\"), \"minutes\" :m.group(\"minutes\"),\"seconds\": m.group(\"seconds\")}\n",
    "    mgroup = {key:int(val) for key, val in mgroup.items() if val is not None}\n",
    "    return datetime.timedelta(**mgroup)\n",
    "    \n",
    "\n",
    "def make_time_intervals(stint, n):\n",
    "    \"\"\"create n contiguous datetime intervals of equal period.\n",
    "    \n",
    "    attributes\n",
    "    ------------\n",
    "    stint : tuple     a pair of datetime objects\n",
    "    n     : integer   the number of time intervals\"\"\"\n",
    "    start, end = stint\n",
    "\n",
    "    in_seconds = (end - start).total_seconds()\n",
    "    period = datetime.timedelta(seconds = in_seconds/n) #interval length, type = timedelta\n",
    "\n",
    "    intervals = [[start+i*period, start+(i+1)*period] for i in range(n)]\n",
    "    return intervals\n",
    "\n",
    "\n",
    "#object creation \n",
    "\n",
    "def partition_channel_history(channelId, publishedAfter, publishedBefore, m):\n",
    "    \"\"\"recursively chop a channel into segments of publishing time that contain at most 50 videos.\n",
    "    Returns a list of date boundary tuples.\"\"\"\n",
    "    \n",
    "    #-----helper function-----\n",
    "    #for this next fn, I couldn't figure out a way to pass pairs upwards through layers of \n",
    "    #recursion without inadvertantly nesting lists many layers deep, so I'll just flatten, \n",
    "    #delete dupes and recreate pairs at the end.\n",
    "    def recur_split(interval, n=2):\n",
    "        d1,d2 = interval\n",
    "        if n==1: #does this ever get run?\n",
    "            return [d1,d2]\n",
    "        else:\n",
    "            if lt_50_vids(channelId, d1, d2): #calls Youtube api\n",
    "                good_interval = [d1,d2]\n",
    "                return good_interval\n",
    "            else:\n",
    "                new_intervals = make_time_intervals([d1,d2], n)\n",
    "                A = [recur_split(i) for i in new_intervals]\n",
    "                return [x for y in A for x in y] #flatten everything by 1 level    \n",
    "    \n",
    "    A = recur_split([publishedAfter, publishedBefore], n=m) #flattened list o.t.f [a,b,b,c,c,d,d,e,e,f]\n",
    "    datetime_segments = [[A[i],A[i+1]] for i in range(0, len(A),2)] #creates pairs[(a,b),(b,c),(c,d)...]\n",
    "    \n",
    "    return datetime_segments\n",
    "\n",
    "def save_history(channelId, history):\n",
    "    \"\"\"save a partitioning of a channels upload history to file\n",
    "    \n",
    "    attributes\n",
    "    ----------\n",
    "    channelId : (string)        Youtube channelId\n",
    "    history   : (list of pairs of datetime objects)   \n",
    "        a partitioning of channel's publishing history into <50 video increments of time \"\"\"\n",
    "    \n",
    "    #make sure all datetimes have identical formats as strings, i.e. microseconds always present\n",
    "    fmt = \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "    formatter = lambda date : datetime.datetime.strftime(date,fmt)\n",
    "    history_formatted = [[formatter(a),formatter(b)] for a,b in history] \n",
    "    \n",
    "    #format as json\n",
    "    history_json = json.dumps(history_formatted)\n",
    "    \n",
    "    filename = \"{0}_partition.txt\".format(channelId)\n",
    "    with open(filename,\"w\") as f:\n",
    "        f.write(history_json)\n",
    "    print \"{0} written to cwd\".format(filename)\n",
    "    \n",
    "def load_history(filename):\n",
    "    \"\"\"load a channel's upload history\"\"\"\n",
    "    with open(filename,\"rb\") as f:\n",
    "        data = json.loads(f.read())\n",
    "        fmt = \"%Y-%m-%d %H:%M:%S.%f\"   #datetime format\n",
    "        p = lambda date: datetime.datetime.strptime(date,fmt) #date formatter\n",
    "        formatted_data = [[p(a),p(b)] for a,b in data]\n",
    "        \n",
    "    return formatted_data\n",
    "\n",
    "def prepare_channel_request(channelIds):\n",
    "    \"\"\"create  youtube API request for channel data.\n",
    "    \n",
    "    Collects both timeseries and meta data\"\"\"\n",
    "    query_params = [(\"part\",[\"snippet\",\"statistics\",\"topicDetails\"]),\n",
    "                    (\"id\",channelIds),\n",
    "                    (\"maxResults\",50),\n",
    "                    (\"key\",apikey)\n",
    "                   ]\n",
    "    s = write_request(\"channels\", query_params)\n",
    "    return s\n",
    "\n",
    "def get_channel_partition(channelId):\n",
    "    \"\"\"Split the channel into a list of time intervals guaranteed to contain less than 50 videos\"\"\"\n",
    "    d1 = ch_creation_date(channelId)\n",
    "    d2 = datetime.datetime.utcnow()\n",
    "    n = ch_v_count(channelId)\n",
    "    m = -(-n // 50) #ceiling \n",
    "    partition =  partition_channel_history(channelId,d1,d2,m)\n",
    "    return partition\n",
    "\n",
    "def get_channel_vids(channelId, channel_partition):\n",
    "    \"\"\"fetch from YT API the video_id of every video ever published by a channel\"\"\"\n",
    "    requests = [make_query_Ch_get_vids_between_dates(channelId,d1,d2) for d1,d2 in channel_partition]\n",
    "    responses = async_fetch(requests)\n",
    "    videos = map(extract_video_ids,responses)\n",
    "#     return videos\n",
    "    return flatten(videos)\n",
    "\n",
    "###################:---database operations---:###################\n",
    "\n",
    "def make_db_connection():\n",
    "    \"\"\"connect to the postgres database, return connection and cursor\"\"\"\n",
    "    conn = psycopg2.connect(\"dbname=postgres user=postgres password=nicholas\")\n",
    "    cur = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)\n",
    "    return conn, cur\n",
    "\n",
    "def upsert_channel_meta(channel_meta, db_connection, db_cursor):\n",
    "    \"\"\"upsert list of channel metadata records to postgreSQL database, return record ids\"\"\"\n",
    "    for channel in channel_meta:\n",
    "        db_cursor.execute(\"INSERT INTO channels_meta (channel_id, published_at, title, description, topic_ids, contains_non_maths_videos, last_update, next_update) VALUES (%(channelId)s, %(publishedAt)s, %(title)s, %(description)s, %(topicIds)s, %(containsNonMathsVideos)s, %(lastUpdate)s, %(nextUpdate)s) ON CONFLICT DO NOTHING;\", channel)\n",
    "    record_ids = db_connection.commit()\n",
    "    #TODO - return database record ids for newly loaded\n",
    "    return record_ids\n",
    "    \n",
    "def stage_channel_timeseries(channel_timeseries, db_connection, db_cursor):\n",
    "    \"\"\"stage time_series data (list of channels_timeseries records) for commit to db\"\"\"\n",
    "    for ts in channel_timeseries:\n",
    "        db_cursor.execute(\"INSERT INTO channels_timeseries (channel_id, sample_time, view_count, comment_count, subscriber_count, video_count) VALUES (%(channelId)s, %(sampleTime)s, %(viewCount)s, %(commentCount)s, %(subscriberCount)s, %(videoCount)s);\", ts)\n",
    "        db_cursor.execute(\"UPDATE channels_meta SET last_update = (%s) where channel_id = (%s);\", (datetime.datetime.utcnow(), ts[\"channelId\"]))\n",
    "    return None\n",
    " \n",
    "        \n",
    "def upsert_video_meta(video_meta, db_connection, db_cursor):\n",
    "    \"\"\"upsert list of video metadata records to postgreSQL database, return record ids\"\"\"\n",
    "    for video in video_meta:\n",
    "        db_cursor.execute(\"INSERT INTO videos_meta (video_id, channel_id, published_at, title, description, tags, duration, last_update, next_update) VALUES (%(videoId)s, %(channelId)s, %(publishedAt)s, %(title)s, %(description)s, %(tags)s, %(duration)s, %(lastUpdate)s, %(nextUpdate)s) ON CONFLICT DO NOTHING;\", video)\n",
    "    record_ids = db_connection.commit()\n",
    "    #TODO - return database record ids for newly loaded\n",
    "    return record_ids\n",
    "\n",
    "def stage_video_timeseries(video_timeseries, db_connection, db_cursor):\n",
    "    \"\"\"update time_series data for list of video_timeseries records\"\"\"\n",
    "    for ts in video_timeseries:\n",
    "        now = datetime.datetime.utcnow()\n",
    "        db_cursor.execute(\"INSERT INTO videos_timeseries (video_id, sample_time, view_count, like_count, dislike_count, favorite_count, comment_count) VALUES (%(videoId)s, %(sampleTime)s, %(viewCount)s, %(likeCount)s, %(dislikeCount)s, %(favoriteCount)s, %(commentCount)s) ON CONFLICT DO NOTHING;\", ts)\n",
    "        db_cursor.execute(\"UPDATE videos_meta SET last_update = (%s) where video_id = (%s);\", (now, ts[\"videoId\"]))\n",
    "        db_cursor.execute(\"UPDATE videos_meta SET next_update = (%s) where video_id = (%s);\", (schedule(ts[\"publishedAt\"]), ts[\"videoId\"]))\n",
    "\n",
    "        \n",
    "def stage_video_meta_upsert(video_meta, db_connection, db_cursor):\n",
    "    \"\"\"upsert list of video metadata records to postgreSQL database, return record ids\"\"\"\n",
    "    for video in video_meta:\n",
    "        db_cursor.execute(\"INSERT INTO videos_meta (video_id, channel_id, published_at, title, description, tags, duration, last_update, next_update) VALUES (%(videoId)s, %(channelId)s, %(publishedAt)s, %(title)s, %(description)s, %(tags)s, %(duration)s, %(lastUpdate)s, %(nextUpdate)s) ON CONFLICT DO NOTHING;\", video)   \n",
    "    return None\n",
    "\n",
    "def fetch_new_videos(channelIds, lastUpdates):\n",
    "    \"\"\"fetch from YT API the video_ids of all videos published since last_update, for each channel\"\"\"\n",
    "    now = datetime.datetime.utcnow()\n",
    "    requests = [make_query_Ch_get_vids_between_dates(channelId,lastUpdate,now) for channelId, lastUpdate in zip(channelIds, lastUpdates)]\n",
    "    responses = async_fetch(requests)\n",
    "    videoIds = map(extract_video_ids,responses)\n",
    "#   return videos\n",
    "    return flatten(videoIds)\n",
    "\n",
    "def load_new_videos(video_meta, db_connection, db_cursor):\n",
    "    \"\"\"create new records in videos_meta table from YT video_meta records\"\"\"\n",
    "    stage_video_meta_upsert(video_meta, db_connection, db_cursor)\n",
    "    db_connection.commit()\n",
    "    return None\n",
    "    \n",
    "def update_videos(db_connection, db_cursor):\n",
    "    \"\"\"collect and write to db a videos_timeseries record for all videos_meta records.\"\"\"\n",
    "    #get video ids from database\n",
    "    now_f = datetime.datetime.strftime(datetime.datetime.utcnow(), \"%Y-%m-%d %H:%M:%S\")\n",
    "    print \"current utc time is {}\".format(now_f)\n",
    "    cur.execute(\"SELECT video_id FROM public.videos_meta WHERE next_update < timestamp %(now)s ;\", {\"now\":now_f})\n",
    "    videos = flatten(cur.fetchall())\n",
    "    \n",
    "    print \"Collecting and storing timeseries records for {} videos:\".format(len(videos))\n",
    "#     print videos\n",
    "    #group and batch for processing\n",
    "    groupsize = 1000\n",
    "    print \"working in groups of {} records.\".format(groupsize)\n",
    "    video_groups = grouper(videos, groupsize)\n",
    "    n=len(video_groups)\n",
    "    for i, v_group in enumerate(video_groups):\n",
    "        print \"########group {} of {}\".format(i+1, n)\n",
    "        __, video_timeseries = get_video_data(v_group)\n",
    "\n",
    "        #stage data for commit to db\n",
    "        stage_video_timeseries(video_timeseries, db_connection, db_cursor)\n",
    "\n",
    "        db_connection.commit()\n",
    "    return None\n",
    "    \n",
    "def update_channels(db_connection, db_cursor):\n",
    "    \"\"\"find, collect and write to db all new videos published since last update of each channels_meta record.\n",
    "    \n",
    "    also create new channel_timeseries record\"\"\"\n",
    "    \n",
    "    #get new videos_meta records, grouped by channelId, as a dict with the channelId as key\n",
    "    db_cursor.execute(\"select channel_id, last_update from public.channels_meta;\")\n",
    "    channelIds, lastUpdates = zip(*db_cursor.fetchall()) \n",
    "    videoIds = fetch_new_videos(channelIds, lastUpdates)\n",
    "    video_meta, __ = get_video_data(videoIds)\n",
    "    sorted_video_meta = sorted(video_meta, key = lambda video: video[\"channelId\"])\n",
    "    groupby_object = groupby(sorted_video_meta, key = lambda video: video[\"channelId\"])\n",
    "    videos_meta_by_channels = {channelId:list(videos) for channelId, videos in groupby_object}\n",
    "    \n",
    "    #get channel_timeseries data\n",
    "    channel_meta, channel_timeseries = get_channel_data(channelIds)\n",
    "    \n",
    "    n = len(video_meta)\n",
    "    print \"{} new videos discovered. Capturing metadata and adding to database.\".format(n)\n",
    "    #write records to db, committing new videos and timeseries for a channel together in 1 transaction\n",
    "    for channel_ts_record in channel_timeseries:\n",
    "        channelId = channel_ts_record[\"channelId\"]\n",
    "        video_meta_records = videos_meta_by_channels.get(channelId) #if no new videos, returns None\n",
    "        stage_channel_timeseries([channel_ts_record], db_connection, db_cursor) \n",
    "        if video_meta_records is not None:\n",
    "            stage_video_meta_upsert(video_meta_records, db_connection, db_cursor)\n",
    "        db_connection.commit()\n",
    "    \n",
    "    return None\n",
    "        \n",
    "        \n",
    "        \n",
    "#scripts\n",
    "\n",
    "##### multi channel video collection\n",
    "# n = len(channels)\n",
    "# for i, ch_id in enumerate(channels):\n",
    "#     print \"############# {0} of {1}\".format(i+1,n)\n",
    "#     print ch_id\n",
    "#     p = get_channel_partition(ch_id)\n",
    "#     print \"{0} time segments created.\".format(len(p))\n",
    "#     vids = get_channel_vids(ch_id, p)\n",
    "#     print \"{0} video ids extracted.\".format(len(vids))\n",
    "#     meta,ts = get_video_data(vids)\n",
    "#     ids = upsert_video_meta(meta, conn, cur)\n",
    "\n",
    "##### database setup \n",
    "# conn = psycopg2.connect(\"dbname= user= password=\")\n",
    "# cur = conn.cursor()\n",
    "# cur.execute(\"CREATE TABLE videos_meta (id serial, video_id text PRIMARY KEY, published_at timestamp, channel_id text, title text, description text, tags text, duration interval, relevant_topic_ids text, last_update timestamp, next_update timestamp, is_maths_content boolean);\")\n",
    "# cur.execute(\"CREATE TABLE channels_meta (id serial, channel_id text PRIMARY KEY, published_at timestamp, title text, description text, topic_ids text, contains_non_maths_videos bool, last_update timestamp, next_update timestamp);\")\n",
    "# cur.execute(\"CREATE TABLE videos_timeseries (id serial PRIMARY KEY, video_id text, sample_time timestamp, view_count integer, like_count integer, dislike_count integer, favorite_count integer, comment_count integer);\")\n",
    "# cur.execute(\"CREATE TABLE channels_timeseries (id serial PRIMARY KEY, channel_id text, sample_time timestamp, view_count integer, comment_count integer, subscriber_count integer, video_count integer);\")\n",
    "# conn.commit()\n",
    "\n",
    "##### check expected number of videos for a channel\n",
    "# for ch in channel:\n",
    "#     print \"{0} : {1}\".format(ch, ch_v_count(ch))\n",
    "\n",
    "\n",
    "exceptions = [{\"id\":\"UCQv3dpUXUWvDFQarHrS5P9A\", \"note\":\"inf loop\"},\n",
    "              {\"id\":\"UCcmfO29cb4k6oVm5YIe19Rw\", \"note\": \"burst error?\"},\n",
    "              {\"id\":\"UCYgL81lc7DOLNhnel1_J6Vg\", \"note\": \"burst error?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn, cur = make_db_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200]\n",
      "[200]\n",
      "[200, 200]\n",
      "16 new videos discovered. Capturing metadata and adding to database.\n"
     ]
    }
   ],
   "source": [
    "update_channels(conn, cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current utc time is 2018-09-14 04:09:15\n",
      "Collecting and storing timeseries records for 130 videos:\n",
      "working in groups of 1000 records.\n",
      "########group 1 of 1\n",
      "[200, 200, 200]\n"
     ]
    }
   ],
   "source": [
    "update_videos(conn, cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-18 09:01:21\n"
     ]
    }
   ],
   "source": [
    "now_f = datetime.datetime.strftime(datetime.datetime.utcnow(), \"%Y-%m-%d %H:%M:%S\")\n",
    "print now_f \n",
    "cur.execute(\"SELECT video_id FROM public.videos_meta WHERE next_update < timestamp %(now)s ;\", {\"now\":now_f})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\n"
     ]
    }
   ],
   "source": [
    "__, channel_timeseries = get_channel_data(channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT video_id FROM public.videos_meta;\")\n",
    "channels = flatten(cur.fetchmany(size=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "x = {\"a\":\"A\"}\n",
    "print len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-20 07:13:09.251000\n"
     ]
    }
   ],
   "source": [
    "example_date = datetime.datetime(2018,7,17,8,12,11)\n",
    "print schedule(example_date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute('''WITH ordered_timeseries as (\n",
    "\t\tselect *\n",
    "\t \tfrom public.videos_timeseries\n",
    " \t\torder by video_id, sample_time\n",
    "\t ),\n",
    "\t views_min_max as (\n",
    "\t\t select video_id,\n",
    "\t\t \t\tfirst(sample_time) as sample_f,\n",
    "\t\t \t\tlast(sample_time) as sample_l,\n",
    "\t\t \t\tfirst(view_count) as view_f,\n",
    "\t\t \t\tlast(view_count) as view_l\n",
    "\t\t from ordered_timeseries\n",
    "\t\t group by video_id\n",
    "\t )\n",
    "SELECT\n",
    "video_id,\n",
    "view_l - view_f as delta_views,\n",
    "(view_l - view_f)/(extract(epoch from (sample_l - sample_f))/86400) as views_per_day\n",
    "from views_min_max\n",
    "where extract(epoch from (sample_l - sample_f)) > 0\n",
    "order by delta_views desc''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yonivids_path = os.path.join(os.path.join(os.getcwd(),os.pardir),\"fullVideosSep2018.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "header = [\"meta\",\"videoId\"]\n",
    "yonivids = pandas.read_csv(yonivids_path, names=header )\n",
    "yonivids = yonivids.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23861"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yonividscol= yonivids.loc[:,\"videoId\"]\n",
    "yonividscol.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yonividslist= list(yonivids.loc[:,\"videoId\"])\n",
    "yonividslist = map(lambda x: str(x).replace(\" \",\"\"), yonividslist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200]\n"
     ]
    }
   ],
   "source": [
    "yonivdata = get_video_data(yonividslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yonivdatameta = yonivdata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'channelId': u'UC4Xt-DUAapAtkfaWWkv4OAw',\n",
       " 'description': u'An introduction to number systems, and how to convert between decimal, binary and hexadecimal numbers. \\nAhora con subt\\xedtulos en espa\\xf1ol, escoge CC. Favor de dejarme feedback y correcciones en los comentarios abajo.',\n",
       " 'duration': datetime.timedelta(0, 1007),\n",
       " 'lastUpdate': datetime.datetime(2018, 9, 13, 7, 3, 50, 290000),\n",
       " 'nextUpdate': datetime.datetime(2018, 9, 18, 7, 3, 50, 290000),\n",
       " 'publishedAt': datetime.datetime(2014, 5, 14, 17, 13, 3),\n",
       " 'tags': [u'binary',\n",
       "  u'hexadecimal',\n",
       "  u'decimal',\n",
       "  u'number system',\n",
       "  u'conversion',\n",
       "  u'Binary Number',\n",
       "  u'hex',\n",
       "  u'binary to decimal'],\n",
       " 'title': u'Number Systems - Converting Decimal, Binary and Hexadecimal',\n",
       " 'videoId': u'aW3qCcH6Dao'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yonivdatameta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upsert_video_meta(yonivdatameta, conn, cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for video_id in yonividslist:\n",
    "    cur.execute(\"UPDATE videos_meta SET is_maths_content = (%s) where video_id = (%s);\", (True, video_id))\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute(\"Select * from public.videos_meta where is_maths_content = true;\")\n",
    "vids = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_vid_ids = [i[1] for i in vids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23590\n",
      "23861\n",
      "23567\n"
     ]
    }
   ],
   "source": [
    "print len(db_vid_ids)\n",
    "print len(yonividslist)\n",
    "print len(yonivdatameta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missed_vids = [i for i in yonividslist if i not in db_vid_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200, 200, 200, 200, 200, 200]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_video_data(missed_vids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hCLfogkqzEk'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed_vids[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "803px",
    "left": "641px",
    "right": "20px",
    "top": "120px",
    "width": "357px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
