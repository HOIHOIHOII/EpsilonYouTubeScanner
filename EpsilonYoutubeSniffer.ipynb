{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook documents the construction of an application that performs three actions:\n",
    "\n",
    "1. Create a database containing snap shots of descriptor statistics for the all the videos on a number of channels on Youtube.\n",
    "\n",
    "2. Generate reports of pertinent information summarising some subselection of the videos retrieved.\n",
    "\n",
    "3. Perform time series analysis of of historical snap-shots.\n",
    "\n",
    "\n",
    "# Part 1\n",
    "\n",
    "\n",
    "Let's build the part of the application that talks to Youtube. There is a quickstart guide at:\n",
    "https://developers.google.com/youtube/v3/quickstart/python\n",
    "but we will mostly follow our own route using \n",
    "https://github.com/youtube/api-samples/blob/master/python/search.py\n",
    "\n",
    "We will also refer to \n",
    "\n",
    "https://developers.google.com/youtube/v3/docs/\n",
    "https://www.forgov.qld.gov.au/file/21896/download?token=mo1SpiZT \n",
    "\n",
    "We want to get a list of statistics for all videos for all channels in a given list. We'll need to make calls to at least 4 different resources:\n",
    "\n",
    "> \n",
    "**Channels**\n",
    "-  Needs: channel_name\n",
    "-  Returns: channel_id\n",
    "> \n",
    "**Videos**\n",
    "-  Needs: video_id\n",
    "-  Returns: Statistics on video\n",
    "> \n",
    "**PlaylistItems**\n",
    "-  Needs: playlist_id (derivable from channel_id)\n",
    "-  Returns: All videos_ids on that playlist\n",
    ">\n",
    "**Search**\n",
    "-  Needs: lots of stuff\n",
    "-  Returns: All videos_ids on a channel\n",
    "    \n",
    "It's probably a good idea to first make an index of all the channel_ids of the channels that we want to survey. Usefully, each of these channel ids are only one character away from the id of the corresponding default playlist, which contains every video uploaded by that channel. \n",
    "\n",
    "There is a stackoverflow question which has a lot of useful information as to how we should structure our calls to the API: https://stackoverflow.com/questions/18953499/youtube-api-to-fetch-all-videos-on-a-channel/20795628\n",
    "\n",
    "From this, and a little consideration,  note a few important facts:\n",
    "1. Each call to an API resource returns at most 50 results, i.e., if a channel has uploaded 6000 videos (e.g., Khan Academy), to return every video_id we must make at least 120 calls to the API.\n",
    "2. You can iterate through this list of 6000 videos, 50 at a time, using the optional next_page token. \n",
    "3. You can actually only get a maximum of 500 videos using the next_page token.\n",
    "4. To get around the 500 max limitation, you can try to restrict your query by date of upload. \n",
    "5. We'll need to do a big initial survey to get the video ids to date. After that the process will shift to maintenance of an existing database and should require much lower volumes of queries. However, If we want up-to-date information on videos, that will require a fair bit of resampling. API call quota may become an issue.\n",
    "\n",
    "\n",
    "There's only a few things that we really want:\n",
    "\n",
    "1. A list of all the videos on a given playlist (-time indexed? only by upload date?)\n",
    "2. Engagement statistics for each video. (-time indexed)\n",
    "\n",
    "\n",
    "This suggests the following tasks:\n",
    "\n",
    "#### Construct index of channel name and channel id. \n",
    "    - [X] Manually construct list of names from Epsilon Stream channel list.\n",
    "    - [X] Store list in file. <span style=\"color:red\"> json_channels.txt</span>\n",
    "    - [ ] Complete channel/playlist details manually\n",
    "\n",
    "#### Build update queue\n",
    "    - [ ] read database, determine which playlists need updating by date\n",
    "    - [ ] load some number of playlist details to memory. construct queue of update requests, write back to database on completion\n",
    "    - [ ] \n",
    "    \n",
    "\n",
    "#### Get list of all video ids on each channel -> Create video-list-updater\n",
    "    - [X] find total number of videos in playlist\n",
    "    - [X] create search request for channel with bounded upload dates to restrict number of results <50\n",
    "    - [X] figure out how to make RFC 3339 timestamps for search reqs\n",
    "    - [X] write function that recursively subdivides upload time until all periods for a channel have < 50 vids\n",
    "    - [ ] create (1-shot?) exclusion list to discard non maths videos\n",
    "    - [X] read search response and obtain all desired details\n",
    "    - [ ] handle aggregation and quality control of requests (e.g. check for doubles/missing, repeat failed requests)\n",
    "    - [ ] handle I/O of results\n",
    "\n",
    "#### Having gotten all video_ids for each channel, for each video:\n",
    "    - [X] Asynchronously query video resource 50 ids at a time \n",
    "    - [ ] routine to extract \"status\" (all details desired) on every video\n",
    "    - [ ] routine to add each \n",
    "    - [ ] program to govern all these actions, reuse/retry function and quality control from earlier\n",
    "\n",
    "\n",
    "    \n",
    "Other TODO:\n",
    "    - [X] Talk to Yoni about database structure. Architecture, basic unit of storage? Learn about NoSQL?\n",
    "    - [X] create table of which resources have which params\n",
    "\n",
    "### Calls to the Youtube API\n",
    "\n",
    "The following API call retrieves the video_ids of the first 50 videos, according to some order, of all videos posted by Khan Academy's channel on youtube. We take advantage of the fact that every channel has a default playlist resource, which contains a list of every video published on that channel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build  \n",
    "#We are currently building an authorisationless app, so we don't need oauth2client\n",
    "import os\n",
    "\n",
    "#put the textfile containing the apikey into the parent folder of your local git repo\n",
    "apikey_path = os.path.join(os.path.join(os.getcwd(),os.pardir),\"apikey.txt\")\n",
    "\n",
    "with open(apikey_path,\"rb\") as f:\n",
    "    apikey = f.readline()\n",
    "\n",
    "DEVELOPER_KEY = apikey\n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asynchronous requests to the Youtube API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nbgrequests as grequests   #changed one tiny boolean\n",
    "import requests\n",
    "import time \n",
    "import os\n",
    "\n",
    "#put the textfile containing the apikey into the parent folder of your local git repo\n",
    "apikey_path = os.path.join(os.path.join(os.getcwd(),os.pardir),\"apikey.txt\")\n",
    "\n",
    "with open(apikey_path,\"rb\") as f:\n",
    "    apikey = f.readline()\n",
    "\n",
    "def tic(t):\n",
    "    # time since t in ms\n",
    "    return (time.time() -t)*1000 \n",
    "\n",
    "# n=2\n",
    "# urls = [url_maker(20+i) for i in range(0,n)]\n",
    "\n",
    "##Example 1: 7 http requests, repeating some targets. Works asynchronously\n",
    "# urls = [\n",
    "#     'http://www.heroku.com',\n",
    "#     'http://python-tablib.org',\n",
    "#     'http://httpbin.org',\n",
    "#     'http://python-requests.org',\n",
    "#     'http://python-requests.org',\n",
    "#     'http://python-requests.org',\n",
    "#     'http://python-requests.org',\n",
    "#       ]\n",
    "\n",
    "###Example 2: 4 https requests, repeating some targets. \n",
    "# urls = [\n",
    "#     'https://www.heroku.com',\n",
    "#     'https://httpbin.org',\n",
    "#     'https://httpbin.org',\n",
    "#     'https://httpbin.org',\n",
    "#       ]\n",
    "\n",
    "###Example 3: 3 Youtube v3 API search queries for the same channel, 3 different search terms. These queries all successfully return results, but do not run asynchronously\n",
    "urls =['https://www.googleapis.com/youtube/v3/search?part=snippet&channelId=UCDWIvJwLJsE4LG1Atne2blQ&key={0}&q=1'.format(apikey)\n",
    "        ,'https://www.googleapis.com/youtube/v3/search?part=snippet&channelId=UCDWIvJwLJsE4LG1Atne2blQ&key={0}&q=2'.format(apikey)\n",
    "        ,'https://www.googleapis.com/youtube/v3/search?part=snippet&channelId=UCDWIvJwLJsE4LG1Atne2blQ&key={0}&q=3'.format(apikey)\n",
    "        ]\n",
    "\n",
    "###Example 4: 3 Youtube v3 API search queries for 3 different channels.\n",
    "# urls = ['https://www.googleapis.com/youtube/v3/search?part=snippet&channelId=UCDWIvJwLJsE4LG1Atne2blQ&key={0}&q=1'.format(apikey)\n",
    "#         ,'https://www.googleapis.com/youtube/v3/search?part=snippet&channelId=UC4a-Gbdw7vOaccHmFo40b9g&key={0}&q=1'.format(apikey)\n",
    "#         ,'https://www.googleapis.com/youtube/v3/search?part=snippet&channelId=UCjwOWaOX-c-NeLnj_YGiNEg&key={0}&q=1'.format(apikey)\n",
    "#         ]\n",
    "\n",
    "\n",
    "def fetch(url):\n",
    "    start = time.time()\n",
    "    response = requests.get(url)\n",
    "    result = response.status_code\n",
    "    print 'Process {}: {:.2f}'.format(url, tic(start))\n",
    "    return result\n",
    "\n",
    "def async_fetch(unsent_requests):\n",
    "    start = time.time()\n",
    "    responses = grequests.map(unsent_requests)\n",
    "    results = [response.status_code for response in responses]\n",
    "    print 'Async Process: {:.2f}'.format( tic(start))\n",
    "    return results\n",
    "\n",
    "def synchronous(urls):\n",
    "    responses = [fetch(i) for i in urls]\n",
    "    return responses\n",
    "        \n",
    "def asynchronous(urls):\n",
    "    unsent_requests = [grequests.get(url) for url in urls]\n",
    "    responses = async_fetch(unsent_requests)\n",
    "    return responses\n",
    "\n",
    "    \n",
    "p_start = time.time()\n",
    "print 'Synchronous:'\n",
    "sync_results = synchronous(urls)\n",
    "print \"All done in {:.2f}\".format(tic(p_start))\n",
    "print sync_results\n",
    "    \n",
    "p_start = time.time()\n",
    "print 'Asynchronous:'\n",
    "async_results = asynchronous(urls)\n",
    "print \"All done in {:.2f}\".format(tic(p_start))   \n",
    "print async_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The async queries are returning in about the time it takes to do one sync query, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating json files\n",
    "\n",
    "We need to choose a method of data storage for all the stuff we'll be getting. JSON might be a good choice - our data is highly structured and we don't know exactly what we want to do with it yet, also we need our data structure to be language independent.\n",
    "\n",
    "Next we'll look at creating and reading json files. This example creates a json file and then reads it back into python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "A = [1,3,4,5,9]\n",
    "\n",
    "#json allows dict, string and list structures\n",
    "json_data = {\"name\":\"Jeff\", \"shifts\" : A, \"title\" : \"Chef\"} \n",
    "\n",
    "dump_list = [json_data for _ in range(10)]\n",
    "\n",
    "#parses object to json format\n",
    "dump = json.dumps(dump_list)\n",
    "\n",
    "#creates file in cwd\n",
    "with open(\"json_dump.txt\",\"wb\") as f:\n",
    "    f.write(dump)\n",
    "\n",
    "with open(\"json_dump.txt\",\"rb\") as f:\n",
    "    everything = f.read()\n",
    "\n",
    "#parses json object to python types\n",
    "jsond = json.loads(everything)\n",
    "\n",
    "print jsond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HEre's a text file with all the current channels added to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(\"json_channels.txt\",\"rb\") as f:\n",
    "    channels_json = json.loads(f.read())\n",
    "    \n",
    "channels_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in channels_json:\n",
    "    print(\"{0:12.12}    {1:5.5}    {2:20.20}    {3}\".format(i[\"channelName\"],i[\"channelId\"],i[\"playListId\"],i[\"comment\"]))\n",
    "#     print i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# general API requests\n",
    "\n",
    "I'll need to do a lot of different queries, some of them will be async and some not. I'd like the async part of all this to look the same as the sync part. How should I define the basic objects and their methods?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What params does each resource have?\n",
    "\n",
    "I'll be pointing my requests to only a few of the many Youtube *resources*. These resources take a set of parameters, some of them have constant meaning across resources while other retain the same name with a different meaning. Other parameters are entirely unique to a resource. The following dictionary describes the params available for each resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resource_params = [\n",
    "    {\n",
    "     \"name\":\"channels\",\n",
    "     \"params\" : [\"part\", \"categoryId\",\"forUsername\",\n",
    "                 \"hl\",\"id\",\"managedByMe\",\n",
    "                 \"maxResults\", \"mine\", \"mySubscribers\",\n",
    "                 \"onBehalfOfContentOwner\",\"pageToken\",\"fields\"\n",
    "                ]\n",
    "    },\n",
    "    {\n",
    "     \"name\":\"videos\",\n",
    "     \"params\":[\"part\", \"chart\", \"hl\",\n",
    "               \"id\", \"locale\", \"maxHeight\",\n",
    "               \"maxWidth\", \"myRating\", \"onBehalfOfContentOwner\",\n",
    "               \"pageToken\", \"regionCode\", \"videoCategoryId\",\n",
    "               \"fields\"\n",
    "              ]\n",
    "    },\n",
    "    {\n",
    "     \"name\":\"search\",\n",
    "     \"params\":[\"part\", \"channelId\", \"channelType\",\n",
    "               \"eventType\", \"forContentOwner\", \"forDeveloper\",\n",
    "               \"forMine\", \"location\", \"locationRadius\",\n",
    "               \"maxResults\", \"onBehalfOfContentOwner\", \"order\",\n",
    "               \"pageToken\", \"publishedAfter\", \"publishedBefore\",\n",
    "               \"q\", \"regionCode\", \"relatedToVideoId\", \"relevanceLanguage\",\n",
    "               \"safeSearch\", \"type\", \"videoCaption\", \"videoCategoryId\",\n",
    "               \"videoDefinition\", \"videoDimension\", \"videoDuration\",\n",
    "               \"videoEmbeddable\", \"videoLicense\", \"videoSyndicated\",\n",
    "               \"videoType\", \"fields\"\n",
    "              ]\n",
    "    },\n",
    "    {\n",
    "     \"name\":\"playlistItems\",\n",
    "     \"params\":[\"part\",\"id\",\"maxResults\", \n",
    "               \"onBehalfOfContentOwner\",\"pageToken\",\n",
    "               \"playlistId\",\"videoId\",\"fields\"\n",
    "              ]\n",
    "    },\n",
    "    {\n",
    "     \"name\":\"playlists\",\n",
    "     \"params\":[\"part\", \"channelId\",\"hl\",\"id\",\n",
    "               \"maxResults\", \"mine\",\"onBehalfOfContentOwner\",\n",
    "               \"onBehalfOfContentOwnerChannel\",\"pageToken\",\"fields\"\n",
    "              ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the functions!\n",
    "\n",
    "Here's the functions we need to create, modify and update our database of youtube videos and channels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.googleapis.com/youtube/v3/playlists?part=contentDetails&id=UUDWIvJwLJsE4LG1Atne2blQ&maxResults=50&key=AIzaSyDYEubHdR-SolXXyXdCbCF1ivVL5sy8k3c\n",
      "294\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import nbgrequests as grequests   #changed one tiny boolean\n",
    "import datetime\n",
    "import itertools\n",
    "import math\n",
    "from __future__ import division\n",
    "\n",
    "\n",
    "#sending queries\n",
    "\n",
    "def send_query(query):\n",
    "    '''gets API response for a single query'''\n",
    "    response = requests.get(query)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def async_fetch(requests):\n",
    "    unsent_requests = [grequests.get(r) for r in requests]\n",
    "    responses = grequests.map(unsent_requests)\n",
    "    status_codes = [response.status_code for response in responses]\n",
    "    print status_codes\n",
    "    results = [response.json() for response in responses]\n",
    "    return results\n",
    "\n",
    "#tools\n",
    "\n",
    "def grouper(items,group_size):\n",
    "    \"\"\"groups list of items into sublists of length group_size\"\"\"\n",
    "    n=len(items)\n",
    "    num_groups = int(math.ceil(n / group_size))\n",
    "    groups = [items[i::num_groups] for i in range(num_groups)]\n",
    "    return groups\n",
    "\n",
    "\n",
    "#query construction\n",
    "\n",
    "def htmlify(s):\n",
    "    \"\"\"alter string to replace html disallowed characters in Youtube API query\"\"\"\n",
    "    substitutions = {\",\":\"%2C\"}\n",
    "    for a,b in substitutions.items():\n",
    "        s = s.replace(a,b)\n",
    "    return s\n",
    "\n",
    "def write_request(resource, params):\n",
    "    \"\"\"take resource name and a list of (key,value) pairs, write a Youtube API request\n",
    "    \n",
    "    attributes\n",
    "    ----------\n",
    "    resource: string                  name of the desired Youtube resource\n",
    "    params  : list of key,val pairs   other parameters for the query  \n",
    "    \n",
    "    params is encoded as a list of key val pairs because code below needed a predictable order\n",
    "    in params and dict doesn't have that.\"\"\"\n",
    "    \n",
    "    query = \"https://www.googleapis.com/youtube/v3/\"    #all queries begin thus\n",
    "    query = query+\"{0}\".format(resource)\n",
    "    \n",
    "    for key, value in params:\n",
    "        if type(value) is list:                         #special handling of lists\n",
    "            comma_sep_vals = \",\".join(value)\n",
    "            term = \"{0}={1}\".format(key,comma_sep_vals)\n",
    "        elif type(value) is datetime.datetime:          #special handling of datetimes, must be RFC3339 format\n",
    "            term = \"{0}={1}\".format(key,to_RFC3339(value))\n",
    "        else:\n",
    "            term = \"{0}={1}\".format(key,value)\n",
    "            \n",
    "        if key == \"part\":\n",
    "            query = query+\"?{0}\".format(term)\n",
    "        else:\n",
    "            query = query+\"&{0}\".format(term)\n",
    "    return htmlify(query)\n",
    "\n",
    "\n",
    "\n",
    "###info fetch functions\n",
    "#build write and read functions for each type of query\n",
    "\n",
    "def make_query_PL_vid_count(playlistId):\n",
    "    '''make a query to get the number of videos in a given playlist'''\n",
    "    \n",
    "    query_params = [(\"part\",[\"contentDetails\"]),\n",
    "                    (\"id\",playlistId),\n",
    "                    (\"maxResults\",50),\n",
    "                    (\"key\",apikey)\n",
    "                   ]\n",
    "    \n",
    "    s = write_request(\"playlists\", query_params)\n",
    "    return s\n",
    "\n",
    "def read_reply_get_PL_vid_count(response):\n",
    "    '''reads PL_vid_count response and returns number of videos in the playlist'''\n",
    "    return response[\"items\"][0][\"contentDetails\"][\"itemCount\"]\n",
    "\n",
    "\n",
    "def make_query_Ch_vid_count(channelId):\n",
    "    '''make a query to get the number of videos in a given playlist'''\n",
    "    \n",
    "    query_params = [(\"part\",[\"statistics\"]),\n",
    "                    (\"id\",channelId),\n",
    "                    (\"maxResults\",50),\n",
    "                    (\"key\",apikey)\n",
    "                   ]\n",
    "#     s = 'https://www.googleapis.com/youtube/v3/channels?part=statistics&id={0}&key={1}'.format(channelId, apikey)\n",
    "    s = write_request(\"channels\", query_params)\n",
    "    return s\n",
    "\n",
    "def read_reply_get_Ch_vid_count(response):\n",
    "    '''reads Ch_vid_count response and returns number of videos in the playlist'''\n",
    "    return response[\"items\"][0][\"statistics\"][\"videoCount\"]\n",
    "\n",
    "def make_query_Ch_srch_response_count_between_dates(channelId, publishedAfter, publishedBefore):\n",
    "    '''make a query to get the number of videos on a given channel between two dates, by counting items in results.\n",
    "    \n",
    "    attributes\n",
    "    ----------\n",
    "    channelId : string       Youtube Channel id\n",
    "    publishedAfter  : datetime    start of date interval\n",
    "    publishedBefore : datetime    end of date interval \n",
    "    \n",
    "    Do not trust totalResults : val in response for the true number of hits. This number is unreliable. \n",
    "    e.g. #results(fn(chId,d1,d2))+ #results(fn(chId,d2,d3)) =/= #results(fn(chId,d1,d3))'''\n",
    "    \n",
    "    query_params = [(\"part\",[\"snippet\"]),\n",
    "                    (\"publishedAfter\", publishedAfter),\n",
    "                    (\"publishedBefore\", publishedBefore),\n",
    "                    (\"channelId\",channelId),\n",
    "                    (\"type\",\"video\"),\n",
    "                    (\"order\",\"date\"),                      #probably not necessary\n",
    "                    (\"maxResults\",50),\n",
    "                    (\"key\",apikey)\n",
    "                   ]\n",
    "#     s = 'https://www.googleapis.com/youtube/v3/channels?part=statistics&id={0}&key={1}'.format(channelId, apikey)\n",
    "    s = write_request(\"search\", query_params)\n",
    "    return s\n",
    "\n",
    "def read_reply_Ch_srch_response_count_between_dates(response):\n",
    "    \"\"\"reads Ch_srch_response_count_between_dates response and gets number of search results\"\"\"\n",
    "    return len(response[\"items\"])    #maxResults for request should be set to 50\n",
    "\n",
    "def lt_50_vids(channelId, publishedAfter, publishedBefore):\n",
    "    q = make_query_Ch_srch_response_count_between_dates(\n",
    "            channelId, publishedAfter, publishedBefore)\n",
    "    response = send_query(q)\n",
    "    n = read_reply_Ch_srch_response_count_between_dates(response)\n",
    "    return n < 50\n",
    "\n",
    "def make_query_get_Ch_creation_date(channelId):\n",
    "    '''make a query to get the creation date of a channel'''\n",
    "    \n",
    "    query_params = [(\"part\",[\"snippet\"]),\n",
    "                    (\"id\",channelId),\n",
    "                    (\"maxResults\",50),\n",
    "                    (\"key\",apikey)\n",
    "                   ]\n",
    "    s = write_request(\"channels\", query_params)\n",
    "#     s = 'https://www.googleapis.com/youtube/v3/channels?part=snippet&id={0}&key={1}'.format(channelId, apikey)\n",
    "    return s\n",
    "\n",
    "def read_reply_get_Ch_creation_date(response):\n",
    "    '''reads get_Ch_creation_date response and returns creation date of a channel as a datetime obj'''\n",
    "    return read_str_RFC3339(response[\"items\"][0][\"snippet\"][\"publishedAt\"])\n",
    "\n",
    "def make_query_get_video_details(video_ids):\n",
    "    \"\"\"fetches viewing statistics for a list of videos.\n",
    "    \n",
    "    attributes\n",
    "    ------------\n",
    "    video_ids : list     list of video ids, each an 11 character string\"\"\"\n",
    "    \n",
    "    \n",
    "    video_ids = \",\".join(video_ids)  #transform to string for query\n",
    "    \n",
    "    query_params = [(\"part\",[\"snippet\",\n",
    "                             \"statistics\",\n",
    "                             \"contentDetails\",\n",
    "                            ]),\n",
    "                    (\"id\", video_ids),\n",
    "                    (\"type\",\"video\"),\n",
    "                    (\"maxResults\",50),\n",
    "                    (\"key\",apikey)\n",
    "                   ]\n",
    "#   s = 'https://www.googleapis.com/youtube/v3/channels?part=statistics&id={0}&key={1}'.format(channelId, apikey)\n",
    "    s = write_request(\"videos\", query_params)\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_new_video_data(response):\n",
    "    \"\"\"takes a response and extracts a list of video objects.\n",
    "    \n",
    "    attributes:\n",
    "    -----------\n",
    "    response: string     \n",
    "    Youtube search query response created by fn make_query_Ch_get_vids_between_dates\"\"\"\n",
    "    new_videos = []\n",
    "    for video in response[\"items\"]:\n",
    "        data = {\"metaData\":{},\"timeSeries\":{}}    #initialise video data\n",
    "\n",
    "        #metaData content\n",
    "        data[\"metaData\"][\"videoId\"] = video[\"id\"][\"videoId\"]\n",
    "        data[\"metaData\"][\"videoName\"] = video[\"snippet\"][\"title\"]\n",
    "        data[\"metaData\"][\"channelId\"] = video[\"snippet\"][\"channelId\"]\n",
    "        data[\"metaData\"][\"channelTitle\"] = video[\"snippet\"][\"channelTitle\"]\n",
    "        data[\"metaData\"][\"publishedAt\"] = read_str_RFC3339(video[\"snippet\"][\"publishedAt\"])\n",
    "        data[\"metaData\"][\"description\"] = video[\"snippet\"][\"description\"]\n",
    "\n",
    "        #no timeSeries content at creation\n",
    "\n",
    "        new_videos.append(data)\n",
    "    return new_videos\n",
    "\n",
    "def make_query_Ch_get_vids_between_dates(channelId, publishedAfter, publishedBefore):\n",
    "    '''make a query to get the videos on a given channel between two dates.\n",
    "    \n",
    "    attributes\n",
    "    ----------\n",
    "    channelId : string       Youtube Channel id\n",
    "    publishedAfter  : datetime    start of date interval\n",
    "    publishedBefore : datetime    end of date interval \n",
    "    \n",
    "    Do not trust totalResults : val in response for the true number of search results.  \n",
    "    e.g. #results(fn(chId,d1,d2))+ #results(fn(chId,d2,d3)) =/= #results(fn(chId,d1,d3))'''\n",
    "    \n",
    "    query_params = [(\"part\",[\"snippet\"]),\n",
    "                    (\"publishedAfter\", publishedAfter),\n",
    "                    (\"publishedBefore\", publishedBefore),\n",
    "                    (\"channelId\",channelId),\n",
    "                    (\"type\",\"video\"),\n",
    "                    (\"maxResults\",50),\n",
    "                    (\"key\",apikey)\n",
    "                   ]\n",
    "#     s = 'https://www.googleapis.com/youtube/v3/channels?part=statistics&id={0}&key={1}'.format(channelId, apikey)\n",
    "    s = write_request(\"search\", query_params)\n",
    "    return s\n",
    "\n",
    "\n",
    "def read_reply_get_video_details(response):\n",
    "    \"\"\"read response carrying details about videos\"\"\"\n",
    "        \n",
    "    def read_item(item):\n",
    "\n",
    "        details=  {\"videoId\" : item[\"id\"],\n",
    "                 \"channelId\" : item[\"snippet\"][\"channelId\"],\n",
    "                 'commentCount': item[\"statistics\"]['commentCount'],\n",
    "                 'dislikeCount': item[\"statistics\"]['dislikeCount'],\n",
    "                 'favoriteCount': item[\"statistics\"]['favoriteCount'],\n",
    "                 'likeCount': item[\"statistics\"]['likeCount'],\n",
    "                 'viewCount': item[\"statistics\"]['viewCount']}\n",
    "        return details\n",
    "        \n",
    "    videos = []\n",
    "    for item in response[\"items\"]:\n",
    "        videos.append(read_item(item))\n",
    "    return videos\n",
    "\n",
    "\n",
    "#datetime manipulation\n",
    "\n",
    "def to_RFC3339(datetime_obj):\n",
    "    \"\"\"format a datetime object in RFC3339 format, e.g. 1999-11-20T04:34:11Z\"\"\"\n",
    "    return datetime_obj.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "def read_str_RFC3339(datetime_obj):\n",
    "    \"\"\"format a datetime object in RFC3339 format, e.g. 1999-11-20T04:34:11Z\"\"\"\n",
    "    return datetime.datetime.strptime(datetime_obj,\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "def make_time_intervals(stint, n):\n",
    "    \"\"\"create n contiguous datetime intervals of equal period.\n",
    "    \n",
    "    attributes\n",
    "    ------------\n",
    "    stint : tuple     a pair of datetime objects\n",
    "    n     : integer   the number of time intervals\"\"\"\n",
    "    start, end = stint\n",
    "    period = (end - start)/n #interval length, type = timedelta\n",
    "    intervals = [[start+i*period, start+(i+1)*period] for i in range(n)]\n",
    "    return intervals\n",
    "\n",
    "#object creation \n",
    "\n",
    "def partition_channel_history(channelId, publishedAfter, publishedBefore, m):\n",
    "    \"\"\"recursively chop a channel into segments of publishing time that contain at most 50 videos.\n",
    "    Returns a list of date boundary tuples.\"\"\"\n",
    "    \n",
    "    #-----helper function-----\n",
    "    #for this next fn, I couldn't figure out a way to pass pairs upwards through layers of \n",
    "    #recursion without inadvertantly nesting lists many layers deep, so I'll just flatten, \n",
    "    #delete dupes and recreate pairs at the end.\n",
    "    def recur_split(interval, n=2):\n",
    "        d1,d2 = interval\n",
    "        if n==1:\n",
    "            return [d1,d2]\n",
    "        else:\n",
    "            if lt_50_vids(channelId, d1, d2): #calls Youtube api\n",
    "                good_interval = [d1,d2]\n",
    "                return good_interval\n",
    "            else:\n",
    "                new_intervals = make_time_intervals([d1,d2], n)\n",
    "                A = [recur_split(i) for i in new_intervals]\n",
    "                return [x for y in A for x in y]            #flatten everything 1 level    \n",
    "    \n",
    "    A = recur_split([publishedAfter, publishedBefore], n=m) #flattened list o.t.f [a,b,b,c,c,d,d,e,e,f]\n",
    "    datetime_segments = [[A[i],A[i+1]] for i in range(0, len(A),2)] #creates pairs[(a,b),(b,c),(c,d)...]\n",
    "    \n",
    "    return datetime_segments\n",
    "\n",
    "def save_history(channelId, history):\n",
    "    \"\"\"save a partitioning of a channels upload history to file\n",
    "    \n",
    "    attributes\n",
    "    ----------\n",
    "    channelId : (string)        Youtube channelId\n",
    "    history   : (list of pairs of datetime objects)   \n",
    "        a partitioning of channel's publishing history into <50 video increments of time \"\"\"\n",
    "    \n",
    "    #make sure all datetimes have identical formats as strings, i.e. microseconds always present\n",
    "    fmt = \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "    formatter = lambda date : datetime.datetime.strftime(date,fmt)\n",
    "    history_formatted = [[formatter(a),formatter(b)] for a,b in history] \n",
    "    \n",
    "    #format as json\n",
    "    history_json = json.dumps(history_formatted)\n",
    "    \n",
    "    filename = \"{0}_partition.txt\".format(channelId)\n",
    "    with open(filename,\"w\") as f:\n",
    "        f.write(history_json)\n",
    "    print \"{0} written to cwd\".format(filename)\n",
    "\n",
    "    \n",
    "def load_history(filename):\n",
    "    \"\"\"load a channel's upload history\"\"\"\n",
    "    with open(filename,\"rb\") as f:\n",
    "        data = json.loads(f.read())\n",
    "        fmt = \"%Y-%m-%d %H:%M:%S.%f\"   #datetime format\n",
    "        p = lambda date: datetime.datetime.strptime(date,fmt) #date formatter\n",
    "        formatted_data = [[p(a),p(b)] for a,b in data]\n",
    "        \n",
    "    return formatted_data\n",
    "\n",
    "# save_history(CId, history)\n",
    "# recovered_history = load_history(\"UC4a-Gbdw7vOaccHmFo40b9g_partition.txt\")   \n",
    "\n",
    "# print recovered_history\n",
    "\n",
    "\n",
    "def create_new_channels(channelIds):\n",
    "    \"\"\"get all channel data required to create a new channel record.\n",
    "    \n",
    "    attributes\n",
    "    -----------\n",
    "    channelIds : list      list of youtube channelIds\"\"\"\n",
    "    \n",
    "    def extract_new_channels_data(response):\n",
    "        new_channels = []\n",
    "        for channel in response[\"items\"]:\n",
    "            data = {\"metaData\":{},\"timeSeries\":{}}    #initialise channel data\n",
    "\n",
    "            #metaData content\n",
    "            data[\"metaData\"][\"channelId\"] = channel[\"id\"]\n",
    "            data[\"metaData\"][\"channelTitle\"] = channel[\"snippet\"][\"title\"]\n",
    "            data[\"metaData\"][\"publishedAt\"] = read_str_RFC3339(channel[\"snippet\"][\"publishedAt\"])\n",
    "            data[\"metaData\"][\"isMixedContentChannel\"] = False #default\n",
    "\n",
    "            #timeSeries content\n",
    "            date = str(datetime.datetime.utcnow())    #youtube uses UTC time\n",
    "            data[\"timeSeries\"][date] = {}             #initialise timeSeries data entry     \n",
    "            if channel[\"statistics\"][\"hiddenSubscriberCount\"] is False:\n",
    "                data[\"timeSeries\"][date][\"subscriberCount\"] = channel[\"statistics\"][\"subscriberCount\"]\n",
    "            \n",
    "            new_channels.append(data)\n",
    "        return new_channels\n",
    "\n",
    "    query_params = [(\"part\",[\"snippet\",\"statistics\"]),\n",
    "                    (\"id\",channelIds),\n",
    "                    (\"maxResults\",50),\n",
    "                    (\"key\",apikey)\n",
    "                   ]\n",
    "    s = write_request(\"channels\", query_params)\n",
    "    r = send_query(s)\n",
    "    channels_data = extract_new_channels_data(r)\n",
    "    return channels_data\n",
    "# CId = \"UC4a-Gbdw7vOaccHmFo40b9g\" #khan academy\n",
    "# response = create_new_channels([CId])\n",
    "\n",
    "\n",
    "\n",
    "q2 = make_query_PL_vid_count(\"UUDWIvJwLJsE4LG1Atne2blQ\")\n",
    "# q = make_query_get_channel_vid_ids(\"UCDWIvJwLJsE4LG1Atne2blQ\")\n",
    "r = send_query(q2)\n",
    "\n",
    "print q2\n",
    "print read_reply_get_PL_vid_count(r)\n",
    "# print q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "str(datetime.MAXYEAR)\n",
    "\n",
    "d = datetime.datetime.utcnow() #youtube uses UTC time\n",
    "k = datetime.datetime(1999,11,20,4,34,11)\n",
    "k2 =  datetime.datetime(1989,11,20,4,34,11)\n",
    "# print d\n",
    "# print k\n",
    "# print k.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "print dt_obj\n",
    "\n",
    "#create evenly spaced dates from a given range\n",
    "\n",
    "   \n",
    "k1 = datetime.datetime(2000,1,1,0,0,0)\n",
    "k2 =  datetime.datetime(2000,1,11,0,0,0)\n",
    "for i in make_time_intervals([k1,k2],2):\n",
    "    print i\n",
    "\n",
    "\n",
    "# q = make_query_Ch_vid_count(\"UCDWIvJwLJsE4LG1Atne2blQ\")\n",
    "# r = send_query(q)\n",
    "# print read_reply_get_Ch_vid_count(r)\n",
    "# q = make_query_get_Ch_creation_date(\"UCDWIvJwLJsE4LG1Atne2blQ\")\n",
    "# r = send_query(q)\n",
    "# print read_reply_get_Ch_creation_date(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "per video \n",
    "key is id\n",
    "value is:\n",
    "{metaData, timeSeries}\n",
    "metData is {name, channel, playlists, duration, maybe a bit more stuf}\n",
    "times series is another array of {time,stats}\n",
    "by array I mean dictionary where key is time.\n",
    "stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#https://www.googleapis.com/youtube/v3/search?part=snippet\n",
    "#&channelId=UCDWIvJwLJsE4LG1Atne2blQ&type=video&maxResults=50\n",
    "#&key={YOUR_API_KEY}\n",
    "\n",
    "#example 2\n",
    "#https://www.googleapis.com/youtube/v3/videos?part=snippet%2Cstatistics\n",
    "#&id=0a799xooy-w%2Cyb5DH9y-rB8%2CQh8hO9j76R4%2CIJ-obdnR_j8%2CPE8NNZG9IYw%2CcSw5R-jdMiI%2CB0A2lDzn3yw\n",
    "#&maxResults=50&key={YOUR_API_KEY}\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "query_params = [(\"part\",\"snippet\"),\n",
    "                (\"channelId\" ,\"UCDWIvJwLJsE4LG1Atne2blQ\"),\n",
    "                (\"type\",\"video\"),\n",
    "                (\"maxResults\",50),\n",
    "                (\"key\",apikey)\n",
    "               ]\n",
    "print \"example 1\"\n",
    "print str(write_request(\"search\",query_params))\n",
    "    \n",
    "    \n",
    "query_params = [(\"part\",[\"snippet\", \"statistics\"]),\n",
    "                (\"id\" , [\"0a799xooy-w\",\"yb5DH9y-rB8\",\"Qh8hO9j76R4\",\"IJ-obdnR_j8\",\"PE8NNZG9IYw\",\"cSw5R-jdMiI\",\"B0A2lDzn3yw\"]),\n",
    "                (\"maxResults\",50),\n",
    "                (\"key\",apikey)\n",
    "               ]\n",
    "print \"example 2\"\n",
    "print write_request(\"videos\",query_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CId = \"UC4a-Gbdw7vOaccHmFo40b9g\" #khan academy\n",
    "d1 = datetime.datetime.strptime(\"2006-11-16T18:22:54.001Z\",\"%Y-%m-%dT%H:%M:%S.%fZ\") #approx creation date khan academy\n",
    "d2 = datetime.datetime.utcnow()\n",
    "\n",
    "m = int(read_reply_get_Ch_vid_count(    \n",
    "            send_query(\n",
    "                make_query_Ch_vid_count(\n",
    "                    CId))))//50  \n",
    "\n",
    "history =  partition_channel_history(CId,d1,d2, m)\n",
    "for i in history:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "save_history(CId, history)\n",
    "recovered_history = load_history(\"UC4a-Gbdw7vOaccHmFo40b9g_partition.txt\")   \n",
    "\n",
    "print recovered_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# async get all videos from channel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200]\n"
     ]
    }
   ],
   "source": [
    "CId = \"UC4a-Gbdw7vOaccHmFo40b9g\" #khan acadmey\n",
    "dates = [[datetime.datetime(2007, 10, 15, 0, 33, 19, 721154), datetime.datetime(2007, 10, 30, 3, 1, 4, 526661)], \n",
    "         [datetime.datetime(2007, 10, 30, 3, 1, 4, 526661), datetime.datetime(2007, 11, 14, 5, 28, 49, 332168)], \n",
    "         [datetime.datetime(2007, 11, 14, 5, 28, 49, 332168), datetime.datetime(2007, 11, 21, 18, 42, 41, 734921)], \n",
    "         [datetime.datetime(2007, 11, 21, 18, 42, 41, 734921), datetime.datetime(2007, 11, 25, 13, 19, 37, 936297)], \n",
    "         [datetime.datetime(2007, 11, 25, 13, 19, 37, 936297), datetime.datetime(2007, 11, 29, 7, 56, 34, 137673)], \n",
    "         [datetime.datetime(2007, 11, 29, 7, 56, 34, 137675), datetime.datetime(2007, 12, 14, 10, 24, 18, 943182)], \n",
    "         [datetime.datetime(2007, 12, 14, 10, 24, 18, 943182), datetime.datetime(2008, 1, 13, 15, 19, 48, 554196)], \n",
    "         [datetime.datetime(2008, 1, 13, 15, 19, 48, 554196), datetime.datetime(2008, 2, 12, 20, 15, 18, 165210)], \n",
    "         [datetime.datetime(2008, 2, 12, 20, 15, 18, 165210), datetime.datetime(2008, 3, 14, 1, 10, 47, 776224)], \n",
    "         [datetime.datetime(2008, 3, 14, 1, 10, 47, 776224), datetime.datetime(2008, 4, 13, 6, 6, 17, 387238)], \n",
    "         [datetime.datetime(2008, 4, 13, 6, 6, 17, 387238), datetime.datetime(2008, 4, 28, 8, 34, 2, 192745)], \n",
    "         [datetime.datetime(2008, 4, 28, 8, 34, 2, 192745), datetime.datetime(2008, 5, 13, 11, 1, 46, 998252)], \n",
    "         [datetime.datetime(2008, 5, 13, 11, 1, 46, 998252), datetime.datetime(2008, 6, 12, 15, 57, 16, 609266)], \n",
    "         [datetime.datetime(2008, 6, 12, 15, 57, 16, 609266), datetime.datetime(2008, 7, 12, 20, 52, 46, 220280)], \n",
    "         [datetime.datetime(2008, 7, 12, 20, 52, 46, 220280), datetime.datetime(2008, 8, 12, 1, 48, 15, 831294)], \n",
    "         [datetime.datetime(2008, 8, 12, 1, 48, 15, 831294), datetime.datetime(2008, 8, 27, 4, 16, 0, 636801)], \n",
    "         [datetime.datetime(2008, 8, 27, 4, 16, 0, 636801), datetime.datetime(2008, 9, 11, 6, 43, 45, 442308)]]\n",
    "\n",
    "\n",
    "requests = [make_query_Ch_get_vids_between_dates(CId,d1,d2) for d1,d2 in dates]\n",
    "\n",
    "responses = async_fetch(requests)\n",
    "videos = map(extract_new_video_data,responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "video_ids = map(lambda x: x[\"metaData\"][\"videoId\"],[vid for set_of_vids in videos for vid in set_of_vids])\n",
    "# print video_ids\n",
    "\n",
    "video_groups =  grouper(video_ids,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def read_record_temp(record, attribute):\n",
    "#     \"\"\"reads a  video record and returns metadata value for attribute. \n",
    "#         temporary function.\n",
    "     \n",
    "#     record       {'timeSeries': {}, \n",
    "#                   'metaData': {'videoName':, \n",
    "#                                'description':, \n",
    "#                                'channelId':, \n",
    "#                                'videoId':, \n",
    "#                                'publishedAt': , \n",
    "#                                'channelTitle': }}\n",
    "                               \n",
    "#     \"\"\"\n",
    "    \n",
    "#     return record[\"metaData\"][attribute]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Resp = async_fetch([make_query_get_video_details([video_groups[0][0]])])#[0][\"items\"][0][\"statistics\"]\n",
    "\n",
    "\n",
    "# print Resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200, 200, 200, 200, 200, 200, 200, 200, 200]\n"
     ]
    }
   ],
   "source": [
    "jobs = [make_query_get_video_details(group) for group in video_groups]\n",
    "\n",
    "\n",
    "results = async_fetch(jobs)\n",
    "\n",
    "data = read_reply_get_video_details(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = client.videos_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "videos_collection = db.videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "insert_result = videos_collection.insert_many(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One post: [ObjectId('5b0f976a90e3a016901e2f92'), ObjectId('5b0f976a90e3a016901e2f93'), ObjectId('5b0f976a90e3a016901e2f94'), ObjectId('5b0f976a90e3a016901e2f95'), ObjectId('5b0f976a90e3a016901e2f96'), ObjectId('5b0f976a90e3a016901e2f97'), ObjectId('5b0f976a90e3a016901e2f98'), ObjectId('5b0f976a90e3a016901e2f99'), ObjectId('5b0f976a90e3a016901e2f9a'), ObjectId('5b0f976a90e3a016901e2f9b'), ObjectId('5b0f976a90e3a016901e2f9c'), ObjectId('5b0f976a90e3a016901e2f9d'), ObjectId('5b0f976a90e3a016901e2f9e'), ObjectId('5b0f976a90e3a016901e2f9f'), ObjectId('5b0f976a90e3a016901e2fa0'), ObjectId('5b0f976a90e3a016901e2fa1'), ObjectId('5b0f976a90e3a016901e2fa2'), ObjectId('5b0f976a90e3a016901e2fa3'), ObjectId('5b0f976a90e3a016901e2fa4'), ObjectId('5b0f976a90e3a016901e2fa5'), ObjectId('5b0f976a90e3a016901e2fa6'), ObjectId('5b0f976a90e3a016901e2fa7'), ObjectId('5b0f976a90e3a016901e2fa8'), ObjectId('5b0f976a90e3a016901e2fa9'), ObjectId('5b0f976a90e3a016901e2faa'), ObjectId('5b0f976a90e3a016901e2fab'), ObjectId('5b0f976a90e3a016901e2fac'), ObjectId('5b0f976a90e3a016901e2fad'), ObjectId('5b0f976a90e3a016901e2fae'), ObjectId('5b0f976a90e3a016901e2faf'), ObjectId('5b0f976a90e3a016901e2fb0'), ObjectId('5b0f976a90e3a016901e2fb1'), ObjectId('5b0f976a90e3a016901e2fb2'), ObjectId('5b0f976a90e3a016901e2fb3'), ObjectId('5b0f976a90e3a016901e2fb4'), ObjectId('5b0f976a90e3a016901e2fb5'), ObjectId('5b0f976a90e3a016901e2fb6'), ObjectId('5b0f976a90e3a016901e2fb7'), ObjectId('5b0f976a90e3a016901e2fb8'), ObjectId('5b0f976a90e3a016901e2fb9'), ObjectId('5b0f976a90e3a016901e2fba'), ObjectId('5b0f976a90e3a016901e2fbb'), ObjectId('5b0f976a90e3a016901e2fbc'), ObjectId('5b0f976a90e3a016901e2fbd'), ObjectId('5b0f976a90e3a016901e2fbe'), ObjectId('5b0f976a90e3a016901e2fbf'), ObjectId('5b0f976a90e3a016901e2fc0'), ObjectId('5b0f976a90e3a016901e2fc1'), ObjectId('5b0f976a90e3a016901e2fc2'), ObjectId('5b0f976a90e3a016901e2fc3')]\n"
     ]
    }
   ],
   "source": [
    "print('One post: {0}'.format(insert_result.inserted_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "326px",
    "left": "559.063px",
    "right": "20px",
    "top": "127px",
    "width": "370px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
